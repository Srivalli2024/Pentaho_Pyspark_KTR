{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'C:\\\\KJB_and_KTR\\\\simplepentaho\\\\simplepentaho\\\\Process all tables.kjb'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[1], line 5\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mos\u001b[39;00m \n\u001b[0;32m      4\u001b[0m \u001b[38;5;66;03m# Parse the XML file\u001b[39;00m\n\u001b[1;32m----> 5\u001b[0m tree \u001b[38;5;241m=\u001b[39m \u001b[43mET\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mC:\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mKJB_and_KTR\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msimplepentaho\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43msimplepentaho\u001b[39;49m\u001b[38;5;124;43m\\\u001b[39;49m\u001b[38;5;124;43mProcess all tables.kjb\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      6\u001b[0m root \u001b[38;5;241m=\u001b[39m tree\u001b[38;5;241m.\u001b[39mgetroot()\n\u001b[0;32m      8\u001b[0m \u001b[38;5;66;03m# Find all 'entry' elements\u001b[39;00m\n",
      "File \u001b[1;32mc:\\Users\\BJ819VK\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\xml\\etree\\ElementTree.py:1218\u001b[0m, in \u001b[0;36mparse\u001b[1;34m(source, parser)\u001b[0m\n\u001b[0;32m   1209\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Parse XML document into element tree.\u001b[39;00m\n\u001b[0;32m   1210\u001b[0m \n\u001b[0;32m   1211\u001b[0m \u001b[38;5;124;03m*source* is a filename or file object containing XML data,\u001b[39;00m\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1215\u001b[0m \n\u001b[0;32m   1216\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[0;32m   1217\u001b[0m tree \u001b[38;5;241m=\u001b[39m ElementTree()\n\u001b[1;32m-> 1218\u001b[0m \u001b[43mtree\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mparse\u001b[49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mparser\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1219\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m tree\n",
      "File \u001b[1;32mc:\\Users\\BJ819VK\\AppData\\Local\\Programs\\Python\\Python311\\Lib\\xml\\etree\\ElementTree.py:569\u001b[0m, in \u001b[0;36mElementTree.parse\u001b[1;34m(self, source, parser)\u001b[0m\n\u001b[0;32m    567\u001b[0m close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[0;32m    568\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mhasattr\u001b[39m(source, \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mread\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n\u001b[1;32m--> 569\u001b[0m     source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mopen\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43msource\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mrb\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m    570\u001b[0m     close_source \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[0;32m    571\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'C:\\\\KJB_and_KTR\\\\simplepentaho\\\\simplepentaho\\\\Process all tables.kjb'"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "import os \n",
    "\n",
    "# Parse the XML file\n",
    "tree = ET.parse('C:\\KJB_and_KTR\\simplepentaho\\simplepentaho\\Process all tables.kjb')\n",
    "root = tree.getroot()\n",
    "\n",
    "# Find all 'entry' elements\n",
    "entries = root.findall('.//entry')\n",
    "\n",
    "# Extract the filenames from each entry\n",
    "filenames = []\n",
    "for entry in entries:\n",
    "    filename_element = entry.find('filename')\n",
    "    if filename_element is not None:\n",
    "        filenames.append(filename_element.text)\n",
    "\n",
    "# Print the extracted filenames\n",
    "for filename in filenames:\n",
    "    print(filename)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root tag of 'C:\\KJB_and_KTR\\simplepentaho\\simplepentaho/Get list of tables.ktr': transformation\n",
      "Root tag of 'C:\\KJB_and_KTR\\simplepentaho\\simplepentaho/Process one table.kjb': job\n",
      "Root tag of 'C:\\KJB_and_KTR\\simplepentaho\\simplepentaho/save list of all result files.ktr': transformation\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "# Assume 'filenames' is the list of filenames you obtained from the previous operation\n",
    "filenames = [\n",
    "    \"${Internal.Job.Filename.Directory}/Get list of tables.ktr\",\n",
    "    \"${Internal.Job.Filename.Directory}/Process one table.kjb\",\n",
    "    \"${Internal.Job.Filename.Directory}/save list of all result files.ktr\"\n",
    "]\n",
    "\n",
    "# Replace the placeholder with the actual directory path\n",
    "job_directory = \"C:\\KJB_and_KTR\\simplepentaho\\simplepentaho\"  # Replace with the actual directory path\n",
    "\n",
    "# Process each file\n",
    "for filename in filenames:\n",
    "    # Replace the placeholder with the actual directory path\n",
    "    actual_filename = filename.replace(\"${Internal.Job.Filename.Directory}\", job_directory)\n",
    "    \n",
    "    # Check if the file exists\n",
    "    if os.path.isfile(actual_filename):\n",
    "        # Parse the XML file\n",
    "        tree = ET.parse(actual_filename)\n",
    "        root = tree.getroot()\n",
    "        \n",
    "        # Perform your processing on the root element of each file\n",
    "        # For example, print the root tag name\n",
    "        print(f\"Root tag of '{actual_filename}':\", root.tag)\n",
    "        \n",
    "        # If you need to find specific elements, use root.findall() or other ElementTree functions\n",
    "        # ...\n",
    "    else:\n",
    "        print(f\"File not found: {actual_filename}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "import pyodbc\n",
    "#from openai import AzureOpenAI\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Configure logging with a rotating log file handler\n",
    "log_filename = 'app.log'\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler = RotatingFileHandler(log_filename, maxBytes=10*1024*1024, backupCount=1)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "file_path = None\n",
    "folder_name = None\n",
    "\n",
    "def get_elements_from_file(xml_file_path, element_name):\n",
    "    root = ET.parse(xml_file_path).getroot()\n",
    "    step_elements = root.findall(f'.//{element_name}')\n",
    "    return [ET.tostring(step, encoding='unicode') for step in step_elements] if step_elements else []\n",
    "\n",
    "def save_elements_to_file(input_filename, output_filename):\n",
    "    xml_file_path = f'{input_filename}'\n",
    "    output_text_file_path = f'{output_filename}s.txt' if output_filename == 'step' else 'hop_order.txt'\n",
    "    step_data_strings = get_elements_from_file(xml_file_path, output_filename)\n",
    "\n",
    "    logger.info(f\"{output_filename} elements extracted\")\n",
    "\n",
    "    with open(output_text_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.writelines(f\"{step_data_string}\\n\" for step_data_string in step_data_strings)\n",
    "\n",
    "    logger.info(f\"The output has been saved to '{output_text_file_path}'\")\n",
    "    print(f\"The output has been saved to '{output_text_file_path}'\")\n",
    "\n",
    "def openai_sequence_steps(hop_order_file_path, steps_file_path):\n",
    "    # Read the configuration file for OpenAI credentials\n",
    "    with open('config_file.json') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Set up OpenAI configuration with the loaded credentials\n",
    "    openai.api_type = json_data[\"openai_api_type\"]\n",
    "    openai.api_base = json_data[\"openai_api_base\"]\n",
    "    openai.api_version = json_data[\"openai_api_version\"]\n",
    "    openai.api_key = json_data['openai_api_key']\n",
    "  \n",
    "    # Read the contents of the steps file\n",
    "    with open(hop_order_file_path, 'r', encoding='utf-8') as file:\n",
    "        hop_order = file.read()\n",
    "        \n",
    "    # Read the contents of the steps file\n",
    "    with open(steps_file_path, 'r', encoding='utf-8') as file:\n",
    "        steps = file.read()\n",
    "  \n",
    "    try:\n",
    "        # Make the API call to OpenAI with the hop info and the contents of steps.txt\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"gpt-4-32k\",\n",
    "            temperature=0.1,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert in converting Pentaho code to Spark code.\\n\\nYou will receive a hop order file and a steps file. Analyze the hop order to determine the sequence, then convert the corresponding steps into Spark code following that sequence.\\n\\nReturn the Spark code for the entire flow step by step as per the steps file.\\n\\nGive sequence number to each step.\"\n",
    "                },\n",
    "                {   \"role\": \"user\", \n",
    "                    \"content\": f\"Here is the Pentaho hop order file: {hop_order} and the steps file {steps}\"\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "      \n",
    "        # Extract the output from the response\n",
    "        output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logger.error(f\"{e}\")\n",
    "        raise\n",
    "\n",
    "def copy_specific_files(source_folder, target_folder, file_names):\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        source_path = os.path.join(source_folder, file_name)\n",
    "        target_path = os.path.join(target_folder, file_name)\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy(source_path, target_path)\n",
    "            print(f\"Copied '{file_name}' to '{target_folder}'.\")\n",
    "        else:\n",
    "            print(f\"File '{file_name}' not found in '{source_folder}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_ktr(file_path, folder_name):\n",
    "        input_filename = file_path\n",
    "        folder_name = os.path.join('spark_code', folder_name)\n",
    "        hop_order_file_path = \"hop_order.txt\"\n",
    "        steps_file_path = \"steps.txt\"\n",
    "        output_file_path = \"pyspark_code.txt\"\n",
    "\n",
    "        save_elements_to_file(input_filename, 'order')\n",
    "        save_elements_to_file(input_filename, 'step')\n",
    "\n",
    "        code_output = openai_sequence_steps(hop_order_file_path, steps_file_path)\n",
    "        print(code_output)\n",
    "\n",
    "        with open(output_file_path, 'a', encoding='utf-8') as output_file:\n",
    "            output_file.write(code_output)\n",
    "\n",
    "        source_folder = \"\"\n",
    "        target_folder = folder_name\n",
    "        file_names = [\"hop_order.txt\", \"steps.txt\", \"pyspark_code.txt\"]\n",
    "        copy_specific_files(source_folder, target_folder, file_names)\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(source_folder, file_name)\n",
    "            os.remove(file_path)\n",
    "        logger.info(f\"The converted code is saved in {folder_name} folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 11:31:14,043 - INFO - order elements extracted\n",
      "2024-06-11 11:31:14,046 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-11 11:31:14,048 - INFO - step elements extracted\n",
      "2024-06-11 11:31:14,051 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path is C:\\KJB_and_KTR\\simplepentaho\\Get list of tables.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 11:32:03,136 - INFO - The converted code is saved in spark_code\\Get list of tables folder\n",
      "2024-06-11 11:32:03,149 - INFO - order elements extracted\n",
      "2024-06-11 11:32:03,152 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-11 11:32:03,156 - INFO - step elements extracted\n",
      "2024-06-11 11:32:03,159 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and the steps file, the sequence of steps is as follows:\n",
      "\n",
      "1. get list of tables\n",
      "2. Concat Fields\n",
      "3. Select values\n",
      "4. Copy rows to result\n",
      "5. tables.txt\n",
      "\n",
      "Now, let's convert these steps into Spark code:\n",
      "\n",
      "1. get list of tables\n",
      "\n",
      "```scala\n",
      "// Step 1: get list of tables\n",
      "val jdbcDF = spark.read\n",
      "  .format(\"jdbc\")\n",
      "  .option(\"url\", \"jdbc:mysql://localhost:3306/database\")\n",
      "  .option(\"dbtable\", \"(SELECT * FROM INFORMATION_SCHEMA.TABLES WHERE TABLE_NAME NOT LIKE 'SYSTEM_%') as tables\")\n",
      "  .option(\"user\", \"username\")\n",
      "  .option(\"password\", \"password\")\n",
      "  .load()\n",
      "```\n",
      "\n",
      "2. Concat Fields\n",
      "\n",
      "```scala\n",
      "// Step 2: Concat Fields\n",
      "import org.apache.spark.sql.functions.concat_ws\n",
      "val concatDF = jdbcDF.withColumn(\"TABLE_SCHEMA_NAME\", concat_ws(\".\", $\"TABLE_SCHEMA\", $\"TABLE_NAME\"))\n",
      "```\n",
      "\n",
      "3. Select values\n",
      "\n",
      "```scala\n",
      "// Step 3: Select values\n",
      "val selectDF = concatDF.select($\"TABLE_SCHEMA_NAME\".as(\"tablename\"))\n",
      "```\n",
      "\n",
      "4. Copy rows to result\n",
      "\n",
      "```scala\n",
      "// Step 4: Copy rows to result\n",
      "// In Spark, this step is not necessary as the DataFrame selectDF already holds the result.\n",
      "```\n",
      "\n",
      "5. tables.txt\n",
      "\n",
      "```scala\n",
      "// Step 5: tables.txt\n",
      "selectDF.write.format(\"csv\").option(\"header\", \"true\").save(\"/path/to/tables.txt\")\n",
      "```\n",
      "\n",
      "Please replace the database URL, username, password, and the path to save the tables.txt file with your actual values.\n",
      "Copied 'hop_order.txt' to 'spark_code\\Get list of tables'.\n",
      "Copied 'steps.txt' to 'spark_code\\Get list of tables'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\Get list of tables'.\n",
      "file_path is C:\\KJB_and_KTR\\simplepentaho\\Process one table.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 11:32:38,864 - INFO - The converted code is saved in spark_code\\Process one table folder\n",
      "2024-06-11 11:32:38,876 - INFO - order elements extracted\n",
      "2024-06-11 11:32:38,879 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-11 11:32:38,882 - INFO - step elements extracted\n",
      "2024-06-11 11:32:38,884 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps is as follows:\n",
      "\n",
      "1. Number of rows in ${TABLENAME}\n",
      "2. rows-${TABLENAME}.txt\n",
      "\n",
      "Now, let's convert these steps into Spark code:\n",
      "\n",
      "Step 1: Number of rows in ${TABLENAME}\n",
      "\n",
      "This step is a TableInput type, which means it's reading data from a table. The SQL query is \"SELECT count(*) as NrRows FROM ${TABLENAME}\". This can be converted into Spark code as follows:\n",
      "\n",
      "```scala\n",
      "val tableName = \"your_table_name\" // replace with your table name\n",
      "val df = spark.sql(s\"SELECT count(*) as NrRows FROM $tableName\")\n",
      "```\n",
      "\n",
      "Step 2: rows-${TABLENAME}.txt\n",
      "\n",
      "This step is a TextFileOutput type, which means it's writing the output to a text file. The file name is \"${java.io.tmpdir}/rows-${TABLENAME}.txt\". This can be converted into Spark code as follows:\n",
      "\n",
      "```scala\n",
      "val outputDir = System.getProperty(\"java.io.tmpdir\")\n",
      "df.coalesce(1).write.format(\"text\").option(\"header\", \"true\").save(s\"$outputDir/rows-$tableName.txt\")\n",
      "```\n",
      "\n",
      "So, the complete Spark code is:\n",
      "\n",
      "```scala\n",
      "val tableName = \"your_table_name\" // replace with your table name\n",
      "val df = spark.sql(s\"SELECT count(*) as NrRows FROM $tableName\")\n",
      "val outputDir = System.getProperty(\"java.io.tmpdir\")\n",
      "df.coalesce(1).write.format(\"text\").option(\"header\", \"true\").save(s\"$outputDir/rows-$tableName.txt\")\n",
      "```\n",
      "\n",
      "Please replace \"your_table_name\" with your actual table name.\n",
      "Copied 'hop_order.txt' to 'spark_code\\Process one table'.\n",
      "Copied 'steps.txt' to 'spark_code\\Process one table'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\Process one table'.\n",
      "file_path is C:\\KJB_and_KTR\\simplepentaho\\save list of all result files.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 11:32:59,171 - INFO - The converted code is saved in spark_code\\save list of all result files folder\n",
      "2024-06-11 11:32:59,174 - INFO - order elements extracted\n",
      "2024-06-11 11:32:59,176 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-11 11:32:59,178 - INFO - step elements extracted\n",
      "2024-06-11 11:32:59,180 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps is as follows:\n",
      "\n",
      "1. Get files from result\n",
      "2. Text file output\n",
      "\n",
      "Now, let's convert these steps into Spark code:\n",
      "\n",
      "Step 1: Get files from result\n",
      "In Pentaho, this step is used to get the list of files from the previous result. In Spark, we can use the wholeTextFiles method to get the files from a directory.\n",
      "\n",
      "```scala\n",
      "val files = sc.wholeTextFiles(\"path_to_directory\")\n",
      "```\n",
      "\n",
      "Step 2: Text file output\n",
      "In Pentaho, this step is used to write the result into a text file. In Spark, we can use the saveAsTextFile method to save the result into a text file.\n",
      "\n",
      "```scala\n",
      "files.saveAsTextFile(\"${java.io.tmpdir}files\")\n",
      "```\n",
      "\n",
      "Please replace \"path_to_directory\" with the actual directory path where the files are located. Also, \"${java.io.tmpdir}files\" is the output directory where the result will be saved. If you want to save the result in a different directory, please replace \"${java.io.tmpdir}files\" with your preferred directory path.\n",
      "Copied 'hop_order.txt' to 'spark_code\\save list of all result files'.\n",
      "Copied 'steps.txt' to 'spark_code\\save list of all result files'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\save list of all result files'.\n",
      "file_path is C:\\KJB_and_KTR\\simplepentaho\\set variables.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-11 11:33:32,220 - INFO - The converted code is saved in spark_code\\set variables folder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order file and the steps file, the sequence of steps is as follows:\n",
      "\n",
      "1. Get one tablename\n",
      "2. Set ${TABLENAME}\n",
      "\n",
      "Now, let's convert these steps into Spark code.\n",
      "\n",
      "1. Get one tablename\n",
      "\n",
      "In Pentaho, the \"RowsFromResult\" step is used to get the result of a previous step. In Spark, we can use the \"collect()\" function to get the result of a previous transformation. Assuming that the previous transformation is a DataFrame named \"df\", the equivalent Spark code would be:\n",
      "\n",
      "```scala\n",
      "val tablename = df.collect()\n",
      "```\n",
      "\n",
      "2. Set ${TABLENAME}\n",
      "\n",
      "In Pentaho, the \"SetVariable\" step is used to set a variable that can be used in subsequent steps. In Spark, we can simply assign the result of the previous step to a variable. Assuming that the result of the previous step is stored in the variable \"tablename\", the equivalent Spark code would be:\n",
      "\n",
      "```scala\n",
      "val TABLENAME = tablename\n",
      "```\n",
      "\n",
      "So, the complete Spark code for the entire flow would be:\n",
      "\n",
      "```scala\n",
      "val tablename = df.collect()\n",
      "val TABLENAME = tablename\n",
      "```\n",
      "\n",
      "Please note that this is a simplified conversion and the actual Spark code may vary depending on the context and the specific requirements of your project.\n",
      "Copied 'hop_order.txt' to 'spark_code\\set variables'.\n",
      "Copied 'steps.txt' to 'spark_code\\set variables'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\set variables'.\n"
     ]
    }
   ],
   "source": [
    "def process_directory(directory_path):\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Check if it's a file and not a directory\n",
    "        if os.path.isfile(file_path):\n",
    "            # Process the file based on its extension\n",
    "            if file_path.endswith('.ktr'):\n",
    "                print('file_path is', file_path)\n",
    "                file_name = os.path.basename(file_path)\n",
    "                folder_name = os.path.splitext(file_name)[0]\n",
    "                process_ktr(file_path, folder_name)\n",
    "\n",
    "# Start the processing with the main directory\n",
    "main_directory_path = r'C:\\KJB_and_KTR\\simplepentaho'  # Replace with the path to your directory\n",
    "process_directory(main_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Get list of tables'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_path = r'C:\\KJB_and_KTR\\simplepentaho\\Get list of tables.ktr'\n",
    "file_name = os.path.basename(file_path)\n",
    "folder_name = os.path.splitext(file_name)[0]\n",
    "folder_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "folder_name = os.path.splitext(file_name)[0]\n",
    "folder_name = os.path.splitext(file_names)[0]\n",
    "print(folder_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'spark_code\\\\example'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os# Base directory and filename\n",
    "base_dir = 'user'\n",
    "filename = 'example'\n",
    " \n",
    "# Construct the full path\n",
    "full_path = os.path.join('spark_code', filename)\n",
    "full_path\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "# Base directory where the .ktr files are located\n",
    "base_dir = \"C:/KJB_and_KTR/simplepentaho\"\n",
    "\n",
    "# List of .ktr files\n",
    "ktr_files = []\n",
    "\n",
    "# Function to create a folder for each .ktr file\n",
    "def create_folders_for_ktr(files, base_directory):\n",
    "    for file in files:\n",
    "        # Extract the folder name by removing the '.ktr' extension\n",
    "        folder_name = os.path.splitext(file)[0]\n",
    "        # Create the full path for the new folder\n",
    "        folder_path = os.path.join(base_directory, folder_name)\n",
    "        # Create the folder if it doesn't exist\n",
    "        if not os.path.exists(folder_path):\n",
    "            os.makedirs(folder_path)\n",
    "            print(f\"Folder created: {folder_path}\")\n",
    "        else:\n",
    "            print(f\"Folder already exists: {folder_path}\")\n",
    "\n",
    "# Call the function to create folders\n",
    "create_folders_for_ktr(ktr_files, base_dir)\n",
    "\n",
    "# Add your code here to save content in each folder\n",
    "# For example, to create a text file in each folder:\n",
    "for file in ktr_files:\n",
    "    folder_name = os.path.splitext(file)[0]\n",
    "    folder_path = os.path.join(base_dir, folder_name)\n",
    "    content_file_path = os.path.join(folder_path, \"example.txt\")\n",
    "    with open(content_file_path, \"w\") as content_file:\n",
    "        content_file.write(\"This is an example of content saved in the folder.\")\n",
    "        print(f\"Content saved in: {content_file_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def process_ktr(file_path):\n",
    "   \n",
    "    def openai_sequence_hops_pentaho_logic(question):\n",
    "        f = open('config_file.json')\n",
    "        json_data = json.load(f)\n",
    "\n",
    "        openai.api_type = json_data[\"openai_api_type\"]\n",
    "        openai.api_base = json_data[\"openai_api_base\"]\n",
    "        openai.api_version = json_data[\"openai_api_version\"]\n",
    "        openai.api_key = json_data['openai_api_key']\n",
    "    \n",
    "        try:\n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=\"gpt-4-32k\",\n",
    "                temperature=0.1,\n",
    "                messages= [\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": '''You are an expert pentaho developer. Analyze the provided KJB file. And give me the logic for the KJB File '''\n",
    "    \n",
    "                    },\n",
    "                    {\"role\": \"user\", \"content\": f\"{question}\"},\n",
    "                ]\n",
    "            )\n",
    "        \n",
    "            output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return output\n",
    "            \n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            raise\n",
    "    print(f\"Processing ARM logic for: {file_path}\")\n",
    "\n",
    "def process_kjb(file_path):\n",
    "    \n",
    "    def openai_sequence_steps(hop_order_file_path, steps_file_path):\n",
    "    # Read the configuration file for OpenAI credentials\n",
    "        with open('config_file.json') as f:\n",
    "            json_data = json.load(f)\n",
    "\n",
    "        # Set up OpenAI configuration with the loaded credentials\n",
    "        openai.api_type = json_data[\"openai_api_type\"]\n",
    "        openai.api_base = json_data[\"openai_api_base\"]\n",
    "        openai.api_version = json_data[\"openai_api_version\"]\n",
    "        openai.api_key = json_data['openai_api_key']\n",
    "    \n",
    "        # Read the contents of the steps file\n",
    "        with open(hop_order_file_path, 'r', encoding='utf-8') as file:\n",
    "            hop_order = file.read()\n",
    "            \n",
    "        # Read the contents of the steps file\n",
    "        with open(steps_file_path, 'r', encoding='utf-8') as file:\n",
    "            steps = file.read()\n",
    "    \n",
    "        try:\n",
    "            # Make the API call to OpenAI with the hop info and the contents of steps.txt\n",
    "            response = openai.ChatCompletion.create(\n",
    "                engine=\"gpt-4-32k\",\n",
    "                temperature=0.1,\n",
    "                messages=[\n",
    "                    {\n",
    "                        \"role\": \"system\",\n",
    "                        \"content\": \"You are an expert in converting Pentaho code to Spark code.\\n\\nYou will receive a hop order file and a steps file. Analyze the hop order to determine the sequence, then convert the corresponding steps into Spark code following that sequence.\\n\\nReturn the Spark code for the entire flow step by step as per the steps file.\\n\\nGive sequence number to each step.\"\n",
    "                    },\n",
    "                    {   \"role\": \"user\", \n",
    "                        \"content\": f\"Here is the Pentaho hop order file: {hop_order} and the steps file {steps}\"\n",
    "                    }\n",
    "                ],\n",
    "            )\n",
    "        \n",
    "            # Extract the output from the response\n",
    "            output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "            return output\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "\n",
    "    print(f\"Processing PySpark code for: {file_path}\")\n",
    "\n",
    "def process_directory(directory_path):\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Check if it's a file and not a directory\n",
    "        if os.path.isfile(file_path):\n",
    "            # Process the file based on its extension\n",
    "            if file_path.endswith('.ktr'):\n",
    "                process_ktr(file_path)\n",
    "            elif file_path.endswith('.kjb'):\n",
    "                process_kjb(file_path)\n",
    "\n",
    "# Start the processing with the main directory\n",
    "main_directory_path = 'C:\\KJB_and_KTR\\simplepentaho'  # Replace with the path to your directory\n",
    "process_directory(main_directory_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<function process_directory at 0x00000198F2C02B60>\n"
     ]
    }
   ],
   "source": [
    "print(process_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_directory(directory_path):\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "\n",
    "        # Check if it's a file and not a directory\n",
    "        if os.path.isfile(file_path):\n",
    "            # Process the file based on its extension\n",
    "            if file_path.endswith('.ktr'):\n",
    "                process_ktr(file_path)\n",
    "            elif file_path.endswith('.kjb'):\n",
    "                process_kjb(file_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_ktr(file_path):\n",
    "        input_filename = file_path\n",
    "        folder_name = 'spark_code/' + folder_name\n",
    "        hop_order_file_path = \"hop_order.txt\"\n",
    "        steps_file_path = \"steps.txt\"\n",
    "        output_file_path = \"pyspark_code.txt\"\n",
    "\n",
    "        save_elements_to_file(input_filename, 'order')\n",
    "        save_elements_to_file(input_filename, 'step')\n",
    "\n",
    "        code_output = openai_sequence_steps(hop_order_file_path, steps_file_path)\n",
    "        print(code_output)\n",
    "\n",
    "        with open(output_file_path, 'a', encoding='utf-8') as output_file:\n",
    "            output_file.write(code_output)\n",
    "\n",
    "        source_folder = \"\"\n",
    "        target_folder = folder_name\n",
    "        file_names = [\"hop_order.txt\", \"steps.txt\", \"pyspark_code.txt\"]\n",
    "        copy_specific_files(source_folder, target_folder, file_names)\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(source_folder, file_name)\n",
    "            os.remove(file_path)\n",
    "        logger.info(f\"The converted code is saved in {folder_name} folder\")\n",
    "\n",
    "        folder_path = 'uploads'\n",
    "        files = os.listdir(folder_path)\n",
    "        for file_name in files:\n",
    "            file_path = os.path.join(folder_path, file_name)\n",
    "            os.remove(file_path)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
