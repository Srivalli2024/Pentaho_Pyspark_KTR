Based on the hop order and steps file, the sequence of steps and their corresponding Spark code would be:

1. Get last updated date
```python
last_updated_date = spark.sql("SELECT MAX(last_updated_date) FROM ztob_sportbets")
```

2. Add constants
```python
from pyspark.sql.functions import lit
df = df.withColumn("const_minusone", lit(-1))
```

3. Get max_minusone
```python
from pyspark.sql.functions import expr
df = df.withColumn("max_minusone", expr("date_add(max, const_minusone)"))
```

4. Select field
```python
df = df.select("max_minusone")
```

5. Table input vSapBwSportBets
```python
df = spark.sql("SELECT * FROM vsapbwsportbets WHERE last_updated_date > max_minusone")
```

6. DB lookup ztob_businessday
```python
df = df.join(spark.table("ztob_businessday"), (df.start_time <= df.creation_date) & (df.end_time >= df.creation_date), how='left')
```

7. Replace in string
```python
from pyspark.sql.functions import regexp_replace
df = df.withColumn("market_name_1", regexp_replace(df.market_name_1, "|", " "))
df = df.withColumn("market_name_2", regexp_replace(df.market_name_2, "|", " "))
df = df.withColumn("selection_name_1", regexp_replace(df.selection_name_1, "|", " "))
df = df.withColumn("selection_name_2", regexp_replace(df.selection_name_2, "|", " "))
```

8. String operations
```python
df = df.withColumn("market_name_1", df.market_name_1.trim())
df = df.withColumn("market_name_2", df.market_name_2.trim())
df = df.withColumn("selection_name_1", df.selection_name_1.trim())
df = df.withColumn("selection_name_2", df.selection_name_2.trim())
df = df.withColumn("bet_type", df.bet_type.trim())
```

9. Concat Fields market_name
```python
from pyspark.sql.functions import concat
df = df.withColumn("market_name", concat(df.market_name_1, df.market_name_2))
```

10. Concat Fields selection_name
```python
df = df.withColumn("selection_name", concat(df.selection_name_1, df.selection_name_2))
```

11. Formula
```python
from pyspark.sql.functions import when
df = df.withColumn("multiple_bet", when(df.bet_type == "SGL", "N").otherwise("Y"))
```

12. Duplicate Date & Time fields
```python
df = df.withColumn("creation_date_d", df.creation_date)
df = df.withColumn("creation_date_t", df.creation_date)
```

13. Format Date & Time fields
```python
from pyspark.sql.functions import date_format
df = df.withColumn("business_day", date_format(df.business_day, "yyyyMMdd"))
df = df.withColumn("creation_date_d", date_format(df.creation_date_d, "yyyyMMdd"))
df = df.withColumn("creation_date_t", date_format(df.creation_date_t, "HH:mm:ss"))
```

14. Insert/Update ztob_sportbets
```python
# Assuming df2 is the DataFrame representing the ztob_sportbets table
df2 = df2.alias("a").join(df.alias("b"), (df2.bet_id == df.bet_id) & (df2.match_id == df.match_id) & (df2.creation_date_d == df.creation_date_d) & (df2.creation_date_t == df.creation_date_t), how='left')
for column in df.columns:
    df2 = df2.withColumn(column, when(df2["b." + column].isNull(), df2["a." + column]).otherwise(df2["b." + column]))
df2 = df2.select([col for col in df2.columns if not col.startswith("b.")])
```

15. DB lookup ztob_timetable
```python
df = df.join(spark.table("ztob_timetable"), df.match_id == spark.table("ztob_timetable").match_id, how='left')
```

16. If field value is null
```python
df = df.fillna({"market_no": " ", "sports_type": " "})
```

17. Memory Group by Wager
```python
df = df.groupBy("business_day", "account_no", "bet_type", "channel", "multiple_bet", "bir", "market_no", "match_id", "sports_type").sum("stake", "gst", "sales")
```

18. Avoid update empty row - Wager
```python
df = df.filter(df.business_day.isNotNull() & df.account_no.isNotNull())
```

19. Insert/Update ztob_sportbets_wager
```python
# Assuming df3 is the DataFrame representing the ztob_sportbets_wager table
df3 = df3.alias("a").join(df.alias("b"), (df3.business_day == df.business_day) & (df3.account_no == df.account_no) & (df3.bet_type == df.bet_type) & (df3.channel == df.channel) & (df3.multiple_bet == df.multiple_bet) & (df3.bir == df.bir) & (df3.market_no == df.market_no) & (df3.match_id == df.match_id) & (df3.sports_type == df.sports_type), how='left')
for column in df.columns:
    df3 = df3.withColumn(column, when(df3["b." + column].isNull(), df3["a." + column]).otherwise(df3["b." + column]))
df3 = df3.select([col for col in df3.columns if not col.startswith("b.")])
```

20. OB005.2.1 MapR - ztob_sportbets
```python
df.write.parquet("maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_sportbets", mode="overwrite")
```

21. OB005.2.2 MapR - ztob_sportbets_wager
```python
df3.write.parquet("maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_sportbets_wager", mode="overwrite")
```

Please note that the above Spark code is written in PySpark and assumes that the initial DataFrame is named `df`. Also, the code for the "Insert/Update" steps assumes that the existing tables are loaded into DataFrames named `df2` and `df3` respectively. You may need to adjust the code to fit your specific environment and requirements.