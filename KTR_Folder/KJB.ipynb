{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\BMCS001_JOB_CONTROL_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\BMCS001_JOB_CONTROL.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\code conersion_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\code conersion.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\get_business_date_in_range_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\get_business_date_in_range.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\job_ad_ldap_sync_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\job_ad_ldap_sync.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\JOB_KYC_MAIN_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\JOB_KYC_MAIN.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\JOB_MV_GRU_PREPARE_TEMP_FILE_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\JOB_MV_GRU_PREPARE_TEMP_FILE.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\JOB_OB_MAIN_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\JOB_OB_MAIN.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\JOB_OB_MAIN_MapRDev_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\JOB_OB_MAIN_MapRDev.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\J_MV_ALLProducts_BetSports_Part_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\J_MV_ALLProducts_BetSports_Part.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\J_MV_AllProduct_ES_Merge_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\J_MV_AllProduct_ES_Merge.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\J_MV_LotteryBetType_ES_Merge_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\J_MV_LotteryBetType_ES_Merge.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\J_MV_LotteryDraw_ES_Merge_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\J_MV_LotteryDraw_ES_Merge.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\MM2001_MAPR_RAW_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\MM2001_MAPR_RAW.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\MV_CustomerSecureMasked_get_system_date_mod_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\MV_CustomerSecureMasked_get_system_date_mod.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\OB001_JOB_CONTROL_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\OB001_JOB_CONTROL.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\OB001_MAPR_RAW_kjb.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\OB001_MAPR_RAW.kjb\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_MV_InvoiceConsignee_get_system_date_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_MV_InvoiceConsignee_get_system_date.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_Lottery_adhoc_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_Lottery_adhoc.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_Lottery_daily_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_Lottery_daily.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportBetsSettled_adhoc_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportBetsSettled_adhoc.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportBetsSettled_daily_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportBetsSettled_daily.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportBets_daily_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportBets_daily.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportReport40_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportReport40.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\tr_push_to_postgres_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\tr_push_to_postgres.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\tr_views_priv_remove_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\tr_views_priv_remove.ktr\n",
      "Renamed: C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\tr_views_priv_viewlist_updt_ktr.txt -> C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\tr_views_priv_viewlist_updt.ktr\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "def rename_files_in_folder(folder_path):\n",
    "    for filename in os.listdir(folder_path):\n",
    "        # Full path to the file\n",
    "        file_path = os.path.join(folder_path, filename)\n",
    "        \n",
    "        # Ensure it is a file\n",
    "        if os.path.isfile(file_path):\n",
    "            # Remove the .txt extension\n",
    "            name, ext = os.path.splitext(filename)\n",
    "            if ext == '.txt':\n",
    "                # Check if the prefix contains _ktr and rename accordingly\n",
    "                if '_ktr' in name:\n",
    "                    new_name = name.replace('_ktr', '') + '.ktr'\n",
    "                elif '_kjb' in name:\n",
    "                    new_name = name.replace('_kjb', '') + '.kjb'\n",
    "                else:\n",
    "                    new_name = name\n",
    "                \n",
    "                # Full new path\n",
    "                new_file_path = os.path.join(folder_path, new_name)\n",
    "                \n",
    "                # Rename the file\n",
    "                os.rename(file_path, new_file_path)\n",
    "                print(f'Renamed: {file_path} -> {new_file_path}')\n",
    "\n",
    "# Example usage\n",
    "folder_path = 'C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy'\n",
    "rename_files_in_folder(folder_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import openai\n",
    "import os\n",
    "import shutil\n",
    "import xml.etree.ElementTree as ET\n",
    "import requests\n",
    "import logging\n",
    "from logging.handlers import RotatingFileHandler\n",
    "\n",
    "# Configure logging with a rotating log file handler\n",
    "log_filename = 'app.log'\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(levelname)s - %(message)s')\n",
    "handler = RotatingFileHandler(log_filename, maxBytes=10*1024*1024, backupCount=1)\n",
    "handler.setFormatter(logging.Formatter('%(asctime)s - %(levelname)s - %(message)s'))\n",
    "logger = logging.getLogger(__name__)\n",
    "logger.addHandler(handler)\n",
    "\n",
    "file_path = None\n",
    "folder_name = None\n",
    "\n",
    "def get_elements_from_file(xml_file_path, element_name):\n",
    "    root = ET.parse(xml_file_path).getroot()\n",
    "    step_elements = root.findall(f'.//{element_name}')\n",
    "    return [ET.tostring(step, encoding='unicode') for step in step_elements] if step_elements else []\n",
    "\n",
    "def save_elements_to_file(input_filename, output_filename):\n",
    "    xml_file_path = f'{input_filename}'\n",
    "    output_text_file_path = f'{output_filename}s.txt' if output_filename == 'step' else 'hop_order.txt'\n",
    "    step_data_strings = get_elements_from_file(xml_file_path, output_filename)\n",
    "\n",
    "    logger.info(f\"{output_filename} elements extracted\")\n",
    "\n",
    "    with open(output_text_file_path, 'w', encoding='utf-8') as output_file:\n",
    "        output_file.writelines(f\"{step_data_string}\\n\" for step_data_string in step_data_strings)\n",
    "\n",
    "    logger.info(f\"The output has been saved to '{output_text_file_path}'\")\n",
    "    print(f\"The output has been saved to '{output_text_file_path}'\")\n",
    "\n",
    "def openai_sequence_steps(hop_order_file_path, steps_file_path):\n",
    "    # Read the configuration file for OpenAI credentials\n",
    "    with open('config_file.json') as f:\n",
    "        json_data = json.load(f)\n",
    "\n",
    "    # Set up OpenAI configuration with the loaded credentials\n",
    "    openai.api_type = json_data[\"openai_api_type\"]\n",
    "    openai.api_base = json_data[\"openai_api_base\"]\n",
    "    openai.api_version = json_data[\"openai_api_version\"]\n",
    "    openai.api_key = json_data['openai_api_key']\n",
    "  \n",
    "    # Read the contents of the steps file\n",
    "    with open(hop_order_file_path, 'r', encoding='utf-8') as file:\n",
    "        hop_order = file.read()\n",
    "        \n",
    "    # Read the contents of the steps file\n",
    "    with open(steps_file_path, 'r', encoding='utf-8') as file:\n",
    "        steps = file.read()\n",
    "  \n",
    "    try:\n",
    "        # Make the API call to OpenAI with the hop info and the contents of steps.txt\n",
    "        response = openai.ChatCompletion.create(\n",
    "            engine=\"gpt-4-32k\",\n",
    "            temperature=0.1,\n",
    "            messages=[\n",
    "                {\n",
    "                    \"role\": \"system\",\n",
    "                    \"content\": \"You are an expert in converting Pentaho code to Spark code.\\n\\nYou will receive a hop order file and a steps file. Analyze the hop order to determine the sequence, then convert the corresponding steps into Spark code following that sequence.\\n\\nReturn the Spark code for the entire flow step by step as per the steps file.\\n\\nGive sequence number to each step.\"\n",
    "                },\n",
    "                {   \"role\": \"user\", \n",
    "                    \"content\": f\"Here is the Pentaho hop order file: {hop_order} and the steps file {steps}\"\n",
    "                }\n",
    "            ],\n",
    "        )\n",
    "      \n",
    "        # Extract the output from the response\n",
    "        output = response[\"choices\"][0][\"message\"][\"content\"]\n",
    "        return output\n",
    "    except Exception as e:\n",
    "        print(e)\n",
    "        logger.error(f\"{e}\")\n",
    "        raise\n",
    "\n",
    "def copy_specific_files(source_folder, target_folder, file_names):\n",
    "    if not os.path.exists(target_folder):\n",
    "        os.makedirs(target_folder)\n",
    "    \n",
    "    for file_name in file_names:\n",
    "        source_path = os.path.join(source_folder, file_name)\n",
    "        target_path = os.path.join(target_folder, file_name)\n",
    "        if os.path.exists(source_path):\n",
    "            shutil.copy(source_path, target_path)\n",
    "            print(f\"Copied '{file_name}' to '{target_folder}'.\")\n",
    "        else:\n",
    "            print(f\"File '{file_name}' not found in '{source_folder}'.\")\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def process_ktr(file_path, folder_name):\n",
    "        input_filename = file_path\n",
    "        folder_name = os.path.join('spark_code', folder_name)\n",
    "        hop_order_file_path = \"hop_order.txt\"\n",
    "        steps_file_path = \"steps.txt\"\n",
    "        output_file_path = \"pyspark_code.txt\"\n",
    "\n",
    "        save_elements_to_file(input_filename, 'order')\n",
    "        save_elements_to_file(input_filename, 'step')\n",
    "\n",
    "        code_output = openai_sequence_steps(hop_order_file_path, steps_file_path)\n",
    "        print(code_output)\n",
    "\n",
    "        with open(output_file_path, 'a', encoding='utf-8') as output_file:\n",
    "            output_file.write(code_output)\n",
    "\n",
    "        source_folder = \"\"\n",
    "        target_folder = folder_name\n",
    "        file_names = [\"hop_order.txt\", \"steps.txt\", \"pyspark_code.txt\"]\n",
    "        copy_specific_files(source_folder, target_folder, file_names)\n",
    "        for file_name in file_names:\n",
    "            file_path = os.path.join(source_folder, file_name)\n",
    "            os.remove(file_path)\n",
    "        logger.info(f\"The converted code is saved in {folder_name} folder\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:13:30,488 - INFO - order elements extracted\n",
      "2024-06-13 11:13:30,492 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:13:30,555 - INFO - step elements extracted\n",
      "2024-06-13 11:13:30,560 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\code conersion.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:14:51,337 - INFO - The converted code is saved in spark_code\\code conersion folder\n",
      "2024-06-13 11:14:51,342 - INFO - order elements extracted\n",
      "2024-06-13 11:14:51,346 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:14:51,353 - INFO - step elements extracted\n",
      "2024-06-13 11:14:51,357 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps and corresponding Spark code would be as follows:\n",
      "\n",
      "1. BetType 2\n",
      "```python\n",
      "# Step 1: BetType 2\n",
      "df_BetType2 = spark.read.format('com.crealytics.spark.excel') \\\n",
      "    .option('useHeader', 'true') \\\n",
      "    .option('inferSchema', 'true') \\\n",
      "    .load('${input_path}\\BetType.xls')\n",
      "```\n",
      "\n",
      "2. Select values\n",
      "```python\n",
      "# Step 2: Select values\n",
      "df_SelectValues = df_BetType2.select('BETTYPE', df_BetType2['Bet Type Description '].alias('BetTypeDescription'))\n",
      "```\n",
      "\n",
      "3. BetType\n",
      "```python\n",
      "# Step 3: BetType\n",
      "df_SelectValues.write.parquet('maprfs:///raw/common/MasterDataCodeDescription/BetType', mode='overwrite')\n",
      "```\n",
      "\n",
      "4. Microsoft Excel Input 2 2\n",
      "```python\n",
      "# Step 4: Microsoft Excel Input 2 2\n",
      "df_MicrosoftExcelInput22 = spark.read.format('com.crealytics.spark.excel') \\\n",
      "    .option('useHeader', 'true') \\\n",
      "    .option('inferSchema', 'true') \\\n",
      "    .load('${input_path}\\Commingling.xls')\n",
      "```\n",
      "\n",
      "5. Replace in string\n",
      "```python\n",
      "# Step 5: Replace in string\n",
      "from pyspark.sql.functions import regexp_replace\n",
      "df_ReplaceInString = df_MicrosoftExcelInput22.withColumn('MeetingDate', regexp_replace('MeetingDate', ' 00:00:00.000', ''))\n",
      "```\n",
      "\n",
      "6. Select values 2 2\n",
      "```python\n",
      "# Step 6: Select values 2 2\n",
      "df_SelectValues22 = df_ReplaceInString.select('MeetingDate', 'ClubMeeting', 'ComminglingIndicater')\n",
      "```\n",
      "\n",
      "7. Commingling\n",
      "```python\n",
      "# Step 7: Commingling\n",
      "df_SelectValues22.write.parquet('maprfs:///raw/common/MasterDataCodeDescription/Commingling', mode='overwrite')\n",
      "```\n",
      "\n",
      "8. Microsoft Excel Input 2 2 2\n",
      "```python\n",
      "# Step 8: Microsoft Excel Input 2 2 2\n",
      "df_MicrosoftExcelInput222 = spark.read.format('com.crealytics.spark.excel') \\\n",
      "    .option('useHeader', 'true') \\\n",
      "    .option('inferSchema', 'true') \\\n",
      "    .load('${input_path}\\Location.xls')\n",
      "```\n",
      "\n",
      "9. Replace in string 2\n",
      "```python\n",
      "# Step 9: Replace in string 2\n",
      "df_ReplaceInString2 = df_MicrosoftExcelInput222.withColumn('LocationID', regexp_replace('LocationID', '.0', ''))\n",
      "```\n",
      "\n",
      "10. Select values 2 2 2\n",
      "```python\n",
      "# Step 10: Select values 2 2 2\n",
      "df_SelectValues222 = df_ReplaceInString2.select(df_ReplaceInString2['Location Type '].alias('LocationType'), 'LocationID')\n",
      "```\n",
      "\n",
      "11. Location\n",
      "```python\n",
      "# Step 11: Location\n",
      "df_SelectValues222.write.parquet('maprfs:///raw/common/MasterDataCodeDescription/Location', mode='overwrite')\n",
      "```\n",
      "\n",
      "12. Microsoft Excel Input 2\n",
      "```python\n",
      "# Step 12: Microsoft Excel Input 2\n",
      "df_MicrosoftExcelInput2 = spark.read.format('com.crealytics.spark.excel') \\\n",
      "    .option('useHeader', 'true') \\\n",
      "    .option('inferSchema', 'true') \\\n",
      "    .load('${input_path}\\ClubID.xls')\n",
      "```\n",
      "\n",
      "13. ClubID\n",
      "```python\n",
      "# Step 13: ClubID\n",
      "df_MicrosoftExcelInput2.write.parquet('maprfs:///raw/common/MasterDataCodeDescription/ClubID', mode='overwrite')\n",
      "```\n",
      "\n",
      "14. Microsoft Excel Input 2 2 2 2\n",
      "```python\n",
      "# Step 14: Microsoft Excel Input 2 2 2 2\n",
      "df_MicrosoftExcelInput2222 = spark.read.format('com.crealytics.spark.excel') \\\n",
      "    .option('useHeader', 'true') \\\n",
      "    .option('inferSchema', 'true') \\\n",
      "    .load('${input_path}\\Subsume.xls')\n",
      "```\n",
      "\n",
      "15. Replace in string 3\n",
      "```python\n",
      "# Step 15: Replace in string 3\n",
      "df_ReplaceInString3 = df_MicrosoftExcelInput2222.withColumn('Meeting Date', regexp_replace('Meeting Date', ' 00:00:00.000', '')) \\\n",
      "    .withColumn('Rec No', regexp_replace('Rec No', '.0', ''))\n",
      "```\n",
      "\n",
      "16. Select values 2 2 2 2\n",
      "```python\n",
      "# Step 16: Select values 2 2 2 2\n",
      "df_SelectValues2222 = df_ReplaceInString3.select(df_ReplaceInString3['Rec No'].alias('RecNo'), df_ReplaceInString3['Meeting Date'].alias('MeetingDate'), df_ReplaceInString3['Club Meeting'].alias('ClubMeeting'), 'Actual Meeting')\n",
      "```\n",
      "\n",
      "17. Subsume\n",
      "```python\n",
      "# Step 17: Subsume\n",
      "df_SelectValues2222.write.parquet('maprfs:///raw/common/MasterDataCodeDescription/Subsume', mode='overwrite')\n",
      "```\n",
      "\n",
      "Please note that the actual Spark code may vary depending on the specific environment and Spark version.\n",
      "Copied 'hop_order.txt' to 'spark_code\\code conersion'.\n",
      "Copied 'steps.txt' to 'spark_code\\code conersion'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\code conersion'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\get_business_date_in_range.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:15:35,283 - INFO - The converted code is saved in spark_code\\get_business_date_in_range folder\n",
      "2024-06-13 11:15:35,290 - INFO - order elements extracted\n",
      "2024-06-13 11:15:35,293 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:15:35,301 - INFO - step elements extracted\n",
      "2024-06-13 11:15:35,306 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps and their corresponding Spark code would be:\n",
      "\n",
      "1. gv-get_business_date\n",
      "```python\n",
      "from_date = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"${from_date}\")\n",
      "to_date = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"${to_date}\")\n",
      "reference_date = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"${reference_date}\")\n",
      "```\n",
      "\n",
      "2. mj-merge_data\n",
      "```python\n",
      "merged_data = from_date.join(to_date, \"id\", \"full_outer\").join(reference_date, \"id\", \"full_outer\")\n",
      "```\n",
      "\n",
      "3. Calculator 3\n",
      "```python\n",
      "from pyspark.sql.functions import expr\n",
      "merged_data = merged_data.withColumn(\"EndDate\", expr(\"date_add(TempEndDate, days)\"))\n",
      "```\n",
      "\n",
      "4. gsi-get_yesterday\n",
      "```python\n",
      "from pyspark.sql.functions import date_sub, current_date\n",
      "yesterday = date_sub(current_date(), 1)\n",
      "```\n",
      "\n",
      "5. frmla-update_variables\n",
      "```python\n",
      "from pyspark.sql.functions import when, col\n",
      "merged_data = merged_data.withColumn(\"flag\", when((col(\"FromDate\").isNotNull()) | (col(\"ToDate\").isNotNull()), \"ad_hoc\").otherwise(\"auto\"))\n",
      "merged_data = merged_data.withColumn(\"FromDate\", when(col(\"FromDate\").isNull(), when(col(\"EndDate\").isNull(), col(\"ReferenceDate\")).otherwise(col(\"EndDate\"))).otherwise(col(\"FromDate\")))\n",
      "merged_data = merged_data.withColumn(\"ToDate\", when(col(\"ToDate\").isNull(), yesterday).otherwise(col(\"ToDate\")))\n",
      "```\n",
      "\n",
      "6. Switch / Case\n",
      "```python\n",
      "auto_data = merged_data.filter(col(\"flag\") == \"auto\")\n",
      "ad_hoc_data = merged_data.filter(col(\"flag\") == \"ad_hoc\")\n",
      "```\n",
      "\n",
      "7. Filter rows 2\n",
      "```python\n",
      "filtered_data = auto_data.filter(col(\"FromDate\") <= col(\"ToDate\"))\n",
      "```\n",
      "\n",
      "8. sv-select_from_to_date\n",
      "```python\n",
      "selected_data = filtered_data.select(\"FromDate\", \"ToDate\")\n",
      "```\n",
      "\n",
      "9. Dummy (do nothing)\n",
      "```python\n",
      "# No action required for this step in Spark\n",
      "```\n",
      "\n",
      "10. Calculator\n",
      "```python\n",
      "from pyspark.sql.functions import datediff\n",
      "selected_data = selected_data.withColumn(\"period\", datediff(col(\"ToDate\"), col(\"FromDate\")))\n",
      "```\n",
      "\n",
      "11. Clone row\n",
      "```python\n",
      "# No direct equivalent in Spark, but can be achieved using a combination of explode and sequence functions\n",
      "```\n",
      "\n",
      "12. Calculator 2\n",
      "```python\n",
      "selected_data = selected_data.withColumn(\"Date\", expr(\"date_add(FromDate, num)\"))\n",
      "```\n",
      "\n",
      "13. Merge Join\n",
      "```python\n",
      "# Assuming gv-parameter step has been executed and resulted in a DataFrame named 'parameters'\n",
      "merged_data = selected_data.join(parameters, \"id\", \"left_outer\")\n",
      "```\n",
      "\n",
      "14. Select values\n",
      "```python\n",
      "selected_data = merged_data.select(\"Date\", \"username\", \"password\", \"proxy_host\", \"proxy_port\")\n",
      "```\n",
      "\n",
      "15. crtr-copy_to_next_step\n",
      "```python\n",
      "# No action required for this step in Spark\n",
      "```\n",
      "\n",
      "Please note that the actual Spark code might vary depending on the exact data types and structures of your data.\n",
      "Copied 'hop_order.txt' to 'spark_code\\get_business_date_in_range'.\n",
      "Copied 'steps.txt' to 'spark_code\\get_business_date_in_range'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\get_business_date_in_range'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\MV_CustomerSecureMasked_get_system_date_mod.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:16:18,658 - INFO - The converted code is saved in spark_code\\MV_CustomerSecureMasked_get_system_date_mod folder\n",
      "2024-06-13 11:16:18,664 - INFO - order elements extracted\n",
      "2024-06-13 11:16:18,667 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:16:18,673 - INFO - step elements extracted\n",
      "2024-06-13 11:16:18,675 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps and their corresponding Spark code would be as follows:\n",
      "\n",
      "1. Business Date 2\n",
      "```python\n",
      "from pyspark.sql import SparkSession\n",
      "spark = SparkSession.builder.getOrCreate()\n",
      "df = spark.read.parquet(\"maprfs://${ABCD_MAPR_HDFS_COMMON}/ztall_businessday/data/ztall_businessday.parquet\")\n",
      "```\n",
      "\n",
      "2. Filter rows 1\n",
      "```python\n",
      "df = df.filter(df.system == 'OB')\n",
      "```\n",
      "\n",
      "3. Select timestamp\n",
      "```python\n",
      "from pyspark.sql.functions import to_timestamp\n",
      "df = df.withColumn(\"start_timestamp\", to_timestamp(df.start_timestamp, 'yyyy/MM/dd HH:mm:ss'))\n",
      "df = df.withColumn(\"end_timestamp\", to_timestamp(df.end_timestamp, 'yyyy/MM/dd HH:mm:ss'))\n",
      "```\n",
      "\n",
      "4. lookup business_date\n",
      "```python\n",
      "# Assuming df2 is the dataframe to join with\n",
      "df = df.join(df2, (df.key >= df2.start_timestamp) & (df.key <= df2.end_timestamp), 'left')\n",
      "```\n",
      "\n",
      "5. generate business_date and cdc 2\n",
      "```python\n",
      "from pyspark.sql.functions import datediff, to_date, lit\n",
      "df = df.withColumn(\"cdc\", datediff(to_date(lit('1986/05/11')), df.business_date) - 5973)\n",
      "```\n",
      "\n",
      "6. Select values 3\n",
      "```python\n",
      "df = df.select(df.fr_date, df.cdc)\n",
      "```\n",
      "\n",
      "7. generate business_date and cdc\n",
      "```python\n",
      "# Assuming dateAdd and dateDiff are defined functions\n",
      "from pyspark.sql.functions import expr\n",
      "df = df.withColumn(\"business_date\", expr(\"dateAdd(fr_date, 'd', range(0, dateDiff(fr_date, current_date()) + 1))\"))\n",
      "df = df.withColumn(\"cdc\", expr(\"dateDiff('1986/05/11', business_date, 'd') - 5973\"))\n",
      "df = df.withColumn(\"group\", lit(\"fr_date\"))\n",
      "```\n",
      "\n",
      "8. format\n",
      "```python\n",
      "df = df.select(df.fr_date, df.cdc, df.business_date, df.group)\n",
      "df = df.withColumn(\"business_date\", df.business_date.cast(\"string\"))\n",
      "df = df.withColumn(\"cdc\", df.cdc.cast(\"integer\"))\n",
      "```\n",
      "\n",
      "9. Filter rows\n",
      "```python\n",
      "df = df.filter(df.business_date.isNotNull())\n",
      "```\n",
      "\n",
      "10. YMD\n",
      "```python\n",
      "from pyspark.sql.functions import year, month, dayofmonth\n",
      "df = df.withColumn(\"year\", year(df.business_date))\n",
      "df = df.withColumn(\"month\", month(df.business_date))\n",
      "df = df.withColumn(\"day\", dayofmonth(df.business_date))\n",
      "```\n",
      "\n",
      "11. Copy rows to result\n",
      "```python\n",
      "# Assuming df_final is the final dataframe\n",
      "df_final = df\n",
      "```\n",
      "\n",
      "Please note that some steps are not included in the sequence as they are not connected to any other steps in the hop order file. Also, some assumptions are made for the steps that require additional information not provided in the steps file.\n",
      "Copied 'hop_order.txt' to 'spark_code\\MV_CustomerSecureMasked_get_system_date_mod'.\n",
      "Copied 'steps.txt' to 'spark_code\\MV_CustomerSecureMasked_get_system_date_mod'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\MV_CustomerSecureMasked_get_system_date_mod'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_MV_InvoiceConsignee_get_system_date.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:17:20,431 - INFO - The converted code is saved in spark_code\\TR_MV_InvoiceConsignee_get_system_date folder\n",
      "2024-06-13 11:17:20,440 - INFO - order elements extracted\n",
      "2024-06-13 11:17:20,444 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:17:20,456 - INFO - step elements extracted\n",
      "2024-06-13 11:17:20,460 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps and their corresponding Spark code would be as follows:\n",
      "\n",
      "1. gv-get_business_date\n",
      "```python\n",
      "fr_date = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"${fr_date}\")\n",
      "to_date = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"${to_date}\")\n",
      "```\n",
      "\n",
      "2. fr-fr_and_to_date\n",
      "```python\n",
      "fr_date = fr_date.filter(fr_date.isNotNull())\n",
      "to_date = to_date.filter(to_date.isNotNull())\n",
      "```\n",
      "\n",
      "3. gsi-yesterday\n",
      "```python\n",
      "from pyspark.sql.functions import date_sub, current_date\n",
      "yesterday = date_sub(current_date(), 1)\n",
      "```\n",
      "\n",
      "4. mjsv-cdc\n",
      "```python\n",
      "from pyspark.sql.functions import datediff\n",
      "from datetime import datetime\n",
      "constant = datetime.strptime('1986/05/11 00:00:00.000', '%Y/%m/%d %H:%M:%S.%f')\n",
      "cdc = datediff(yesterday, constant) - 5973\n",
      "```\n",
      "\n",
      "5. cal-year month day\n",
      "```python\n",
      "from pyspark.sql.functions import year, month, dayofmonth\n",
      "year = year(yesterday)\n",
      "month = month(yesterday)\n",
      "day = dayofmonth(yesterday)\n",
      "```\n",
      "\n",
      "6. so-left pad month day\n",
      "```python\n",
      "month = month.rjust(2, '0')\n",
      "day = day.rjust(2, '0')\n",
      "```\n",
      "\n",
      "7. sv-business_date\n",
      "```python\n",
      "business_date = spark.createDataFrame([(cdc, year, month, day)], [\"cdc\", \"year\", \"month\", \"day\"])\n",
      "```\n",
      "\n",
      "8. calc-days_diff\n",
      "```python\n",
      "days_diff = datediff(to_date, fr_date)\n",
      "```\n",
      "\n",
      "9. cr-clone_row\n",
      "```python\n",
      "clone_row = days_diff\n",
      "```\n",
      "\n",
      "10. calc-business_date\n",
      "```python\n",
      "from pyspark.sql.functions import expr\n",
      "business_date = fr_date.withColumn(\"business_date\", expr(\"date_add(fr_date, num_field)\"))\n",
      "```\n",
      "\n",
      "11. mjsv-cdc_2\n",
      "```python\n",
      "cdc_2 = datediff(business_date, constant) - 5973\n",
      "```\n",
      "\n",
      "12. cal-year month day 2\n",
      "```python\n",
      "year_2 = year(business_date)\n",
      "month_2 = month(business_date)\n",
      "day_2 = dayofmonth(business_date)\n",
      "```\n",
      "\n",
      "13. so-left pad month day 2\n",
      "```python\n",
      "month_2 = month_2.rjust(2, '0')\n",
      "day_2 = day_2.rjust(2, '0')\n",
      "```\n",
      "\n",
      "14. sv-business_date_2\n",
      "```python\n",
      "business_date_2 = spark.createDataFrame([(cdc_2, year_2, month_2, day_2)], [\"cdc\", \"year\", \"month\", \"day\"])\n",
      "```\n",
      "\n",
      "15. dmmy-union_all\n",
      "```python\n",
      "union_all = business_date.union(business_date_2)\n",
      "```\n",
      "\n",
      "16. Write to log 2\n",
      "```python\n",
      "union_all.show()\n",
      "```\n",
      "\n",
      "17. crtr-to_next_steps\n",
      "```python\n",
      "# This step is not applicable in Spark as it is used to pass rows to the next steps in Pentaho.\n",
      "```\n",
      "\n",
      "Please note that the above Spark code is a rough translation of the Pentaho steps and may need adjustments based on the actual data and Spark environment.\n",
      "Copied 'hop_order.txt' to 'spark_code\\TR_MV_InvoiceConsignee_get_system_date'.\n",
      "Copied 'steps.txt' to 'spark_code\\TR_MV_InvoiceConsignee_get_system_date'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\TR_MV_InvoiceConsignee_get_system_date'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_Lottery_adhoc.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:22:03,518 - INFO - The converted code is saved in spark_code\\TR_OB_Lottery_adhoc folder\n",
      "2024-06-13 11:22:03,533 - INFO - order elements extracted\n",
      "2024-06-13 11:22:03,537 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:22:03,547 - INFO - step elements extracted\n",
      "2024-06-13 11:22:03,551 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of operations and corresponding Spark code would be:\n",
      "\n",
      "1. Get Variables\n",
      "```python\n",
      "fr_date = spark.sql(\"SELECT value FROM variables WHERE name = 'fr_date'\")\n",
      "to_date = spark.sql(\"SELECT value FROM variables WHERE name = 'to_date'\")\n",
      "```\n",
      "\n",
      "2. Table input vSapBwLottery\n",
      "```python\n",
      "vsapbwlottery = spark.sql(f\"SELECT * FROM vsapbwlottery WHERE creation_date BETWEEN {fr_date} AND {to_date}\")\n",
      "```\n",
      "\n",
      "3. DB lookup ztob_businessday\n",
      "```python\n",
      "ztob_businessday = spark.sql(\"SELECT * FROM ztob_businessday\")\n",
      "vsapbwlottery = vsapbwlottery.join(ztob_businessday, (vsapbwlottery.creation_date <= ztob_businessday.end_time) & (vsapbwlottery.creation_date >= ztob_businessday.start_time), 'left')\n",
      "```\n",
      "\n",
      "4. Replace in string\n",
      "```python\n",
      "from pyspark.sql.functions import regexp_replace\n",
      "vsapbwlottery = vsapbwlottery.withColumn('bet_name', regexp_replace('bet_name', '|', ' '))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_1', regexp_replace('picks_1', '|', ' '))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_2', regexp_replace('picks_2', '|', ' '))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_3', regexp_replace('picks_3', '|', ' '))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_4', regexp_replace('picks_4', '|', ' '))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_5', regexp_replace('picks_5', '|', ' '))\n",
      "```\n",
      "\n",
      "5. String operations\n",
      "```python\n",
      "from pyspark.sql.functions import trim\n",
      "vsapbwlottery = vsapbwlottery.withColumn('bet_name', trim(vsapbwlottery.bet_name))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_1', trim(vsapbwlottery.picks_1))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_2', trim(vsapbwlottery.picks_2))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_3', trim(vsapbwlottery.picks_3))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_4', trim(vsapbwlottery.picks_4))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks_5', trim(vsapbwlottery.picks_5))\n",
      "```\n",
      "\n",
      "6. Concat Fields picks\n",
      "```python\n",
      "from pyspark.sql.functions import concat_ws\n",
      "vsapbwlottery = vsapbwlottery.withColumn('picks', concat_ws(' ', vsapbwlottery.picks_1, vsapbwlottery.picks_2, vsapbwlottery.picks_3, vsapbwlottery.picks_4, vsapbwlottery.picks_5))\n",
      "```\n",
      "\n",
      "7. Constant sales factor\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn('c_totosf', 0.968804495)\n",
      "vsapbwlottery = vsapbwlottery.withColumn('c_4dbsf', 0.977691588)\n",
      "vsapbwlottery = vsapbwlottery.withColumn('c_4dssf', 0.972523364)\n",
      "vsapbwlottery = vsapbwlottery.withColumn('c_4dbssf', 0.975107476)\n",
      "```\n",
      "\n",
      "8. Formula\n",
      "```python\n",
      "from pyspark.sql.functions import when, col\n",
      "vsapbwlottery = vsapbwlottery.withColumn('bet_status', when(col('status').isin('A', 'S', 'X'), col('status')).otherwise(col('ob_status')))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('product_id', when(col('bet_type').startswith('4'), '009').when(col('bet_type').startswith('L'), '023').otherwise('999'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('sales_amount', when(col('type_stake_1') == 'TOTO', col('c_totosf') * col('total_stake')).when((col('type_stake_1') == '4DBIG') & (col('type_stake_2') == ''), col('c_4dbsf') * col('total_stake')).when((col('type_stake_1') == '') & (col('type_stake_2') == '4DSMALL'), col('c_4dssf') * col('total_stake')).when((col('type_stake_1') == '4DBIG') & (col('type_stake_2') == '4DSMALL'), col('c_4dbssf') * col('total_stake')))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('gst_amount', col('total_stake') - col('sales_amount'))\n",
      "```\n",
      "\n",
      "9. User Defined Java Expression\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn('valid_date', when(col('returns') > 0, col('settled_date')).otherwise(None))\n",
      "```\n",
      "\n",
      "10. Duplicate Date & Time fields\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn('creation_date_d', col('creation_date'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('creation_date_t', col('creation_date'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('settled_date_d', col('settled_date'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('settled_date_t', col('settled_date'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('draw_date_d', col('draw_date'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('draw_date_t', col('draw_date'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('valid_date_d', col('valid_date'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('valid_date_t', col('valid_date'))\n",
      "```\n",
      "\n",
      "11. Format Date & Time fields\n",
      "```python\n",
      "from pyspark.sql.functions import date_format\n",
      "vsapbwlottery = vsapbwlottery.withColumn('business_day', date_format('business_day', 'yyyyMMdd'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('creation_date_d', date_format('creation_date_d', 'yyyyMMdd'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('creation_date_t', date_format('creation_date_t', 'HH:mm:ss'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('settled_date_d', date_format('settled_date_d', 'yyyyMMdd'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('settled_date_t', date_format('settled_date_t', 'HH:mm:ss'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('draw_date_d', date_format('draw_date_d', 'yyyyMMdd'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('draw_date_t', date_format('draw_date_t', 'HH:mm:ss'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('valid_date_d', date_format('valid_date_d', 'yyyyMMdd'))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('valid_date_t', date_format('valid_date_t', 'HH:mm:ss'))\n",
      "```\n",
      "\n",
      "12. If field value is null\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn('es_syndicate_id', when(col('es_syndicate_id').isNull(), 0).otherwise(col('es_syndicate_id')))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('es_syndicate_name', when(col('es_syndicate_name').isNull(), ' ').otherwise(col('es_syndicate_name')))\n",
      "vsapbwlottery = vsapbwlottery.withColumn('es_syndicate_parts', when(col('es_syndicate_parts').isNull(), 0).otherwise(col('es_syndicate_parts')))\n",
      "```\n",
      "\n",
      "13. Filter rows for Cancel\n",
      "```python\n",
      "cancel = vsapbwlottery.filter((col('settled_how') == 'M') & (col('bet_status') == 'X'))\n",
      "```\n",
      "\n",
      "14. Memory Group by Cancel\n",
      "```python\n",
      "from pyspark.sql.functions import sum as _sum, count as _count\n",
      "cancel_grouped = cancel.groupBy('business_day', 'account_no', 'bet_name', 'bet_status', 'channel', 'draw_date_d', 'es_draw_no', 'bet_type', 'auto_pick', 'es_syndicate_parts', 'product_id', 'es_syndicate_id', 'es_syndicate_name').agg(_sum('total_stake').alias('bet_amount'), _sum('bet_stake_1').alias('big_stake_board'), _sum('unit_stake_1').alias('big_stake_unit'), _sum('gst_amount').alias('input_output_gst_amount'), _sum('syndicate_parts').alias('number_of_sold_syndicate_units'), _sum('sales_amount').alias('aggr_sales_amount'), _sum('bet_stake_2').alias('small_stake_board'), _sum('unit_stake_2').alias('small_stake_unit'), _count('*').alias('transaction_count'))\n",
      "```\n",
      "\n",
      "15. Avoid update empty row - Cancel\n",
      "```python\n",
      "cancel_grouped = cancel_grouped.filter((col('business_day').isNotNull()) & (col('account_no').isNotNull()))\n",
      "```\n",
      "\n",
      "16. Insert/Update ztob_lottery_cancel\n",
      "```python\n",
      "cancel_grouped.write.mode('overwrite').insertInto('ztob_lottery_cancel')\n",
      "```\n",
      "\n",
      "17. Filter rows for Valid\n",
      "```python\n",
      "valid = vsapbwlottery.filter((col('bet_status') == 'S') & (col('valid_date_d').isNotNull()) & (col('returns') > 0))\n",
      "```\n",
      "\n",
      "18. Memory Group by Valid\n",
      "```python\n",
      "valid_grouped = valid.groupBy('business_day', 'account_no', 'bet_name', 'bet_status', 'channel', 'draw_date_d', 'es_draw_no', 'bet_type', 'auto_pick', 'es_syndicate_parts', 'product_id', 'es_syndicate_id', 'es_syndicate_name').agg(_sum('returns').alias('returns'), _sum('winnings').alias('winnings'), _count('*').alias('transaction_count'))\n",
      "```\n",
      "\n",
      "19. Avoid update empty row - Valid\n",
      "```python\n",
      "valid_grouped = valid_grouped.filter((col('business_day').isNotNull()) & (col('account_no').isNotNull()))\n",
      "```\n",
      "\n",
      "20. Insert/Update ztob_lottery_valid\n",
      "```python\n",
      "valid_grouped.write.mode('overwrite').insertInto('ztob_lottery_valid')\n",
      "```\n",
      "\n",
      "21. Memory Group by Wager\n",
      "```python\n",
      "wager_grouped = vsapbwlottery.groupBy('business_day', 'account_no', 'bet_name', 'bet_status', 'channel', 'draw_date_d', 'es_draw_no', 'bet_type', 'auto_pick', 'es_syndicate_parts', 'product_id', 'es_syndicate_id', 'es_syndicate_name').agg(_sum('total_stake').alias('bet_amount'), _sum('bet_stake_1').alias('big_stake_board'), _sum('unit_stake_1').alias('big_stake_unit'), _sum('gst_amount').alias('input_output_gst_amount'), _sum('syndicate_parts').alias('number_of_sold_syndicate_units'), _sum('sales_amount').alias('aggr_sales_amount'), _sum('bet_stake_2').alias('small_stake_board'), _sum('unit_stake_2').alias('small_stake_unit'), _count('*').alias('transaction_count'))\n",
      "```\n",
      "\n",
      "22. Avoid update empty row - Wager\n",
      "```python\n",
      "wager_grouped = wager_grouped.filter((col('business_day').isNotNull()) & (col('account_no').isNotNull()))\n",
      "```\n",
      "\n",
      "23. Insert/Update ztob_lottery_wager\n",
      "```python\n",
      "wager_grouped.write.mode('overwrite').insertInto('ztob_lottery_wager')\n",
      "```\n",
      "\n",
      "24. Insert/Update ztob_lottery\n",
      "```python\n",
      "vsapbwlottery.write.mode('overwrite').insertInto('ztob_lottery')\n",
      "```\n",
      "\n",
      "Please note that the above Spark code is written in PySpark and assumes that the SparkSession is already created and named as `spark`. The code also assumes that the data is stored in a Hive table. If the data is stored in a different type of database, the SQL queries and write methods may need to be adjusted accordingly.\n",
      "Copied 'hop_order.txt' to 'spark_code\\TR_OB_Lottery_adhoc'.\n",
      "Copied 'steps.txt' to 'spark_code\\TR_OB_Lottery_adhoc'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\TR_OB_Lottery_adhoc'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_Lottery_daily.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:28:08,555 - INFO - The converted code is saved in spark_code\\TR_OB_Lottery_daily folder\n",
      "2024-06-13 11:28:08,568 - INFO - order elements extracted\n",
      "2024-06-13 11:28:08,568 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:28:08,568 - INFO - step elements extracted\n",
      "2024-06-13 11:28:08,583 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps and their corresponding Spark code would be as follows:\n",
      "\n",
      "1. Get last updated date\n",
      "```python\n",
      "last_updated_date = spark.sql(\"SELECT MAX(last_updated_date) FROM ztob_lottery\")\n",
      "```\n",
      "\n",
      "2. Add constants\n",
      "```python\n",
      "from pyspark.sql.functions import lit\n",
      "last_updated_date = last_updated_date.withColumn(\"const_minusone\", lit(-1))\n",
      "```\n",
      "\n",
      "3. Get max_minusone\n",
      "```python\n",
      "from pyspark.sql.functions import expr\n",
      "last_updated_date = last_updated_date.withColumn(\"max_minusone\", expr(\"max - const_minusone\"))\n",
      "```\n",
      "\n",
      "4. Select field\n",
      "```python\n",
      "last_updated_date = last_updated_date.select(\"max_minusone\")\n",
      "```\n",
      "\n",
      "5. Table input vSapBwLottery\n",
      "```python\n",
      "vsapbwlottery = spark.sql(f\"SELECT * FROM vsapbwlottery WHERE last_updated_date > {last_updated_date.collect()[0][0]}\")\n",
      "```\n",
      "\n",
      "6. DB lookup ztob_businessday\n",
      "```python\n",
      "ztob_businessday = spark.sql(\"SELECT * FROM ztob_businessday\")\n",
      "vsapbwlottery = vsapbwlottery.join(ztob_businessday, (vsapbwlottery.creation_date <= ztob_businessday.creation_date) & (vsapbwlottery.creation_date >= ztob_businessday.creation_date), 'left')\n",
      "```\n",
      "\n",
      "7. Replace in string\n",
      "```python\n",
      "from pyspark.sql.functions import regexp_replace\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"bet_name\", regexp_replace(\"bet_name\", \"\\|\", \" \"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"bet_name\", regexp_replace(\"bet_name\", \"_\", \" \"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_1\", regexp_replace(\"picks_1\", \"\\|\", \" \"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_2\", regexp_replace(\"picks_2\", \"\\|\", \" \"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_3\", regexp_replace(\"picks_3\", \"\\|\", \" \"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_4\", regexp_replace(\"picks_4\", \"\\|\", \" \"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_5\", regexp_replace(\"picks_5\", \"\\|\", \" \"))\n",
      "```\n",
      "\n",
      "8. String operations\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"bet_name\", trim(col(\"bet_name\")))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_1\", trim(col(\"picks_1\")))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_2\", trim(col(\"picks_2\")))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_3\", trim(col(\"picks_3\")))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_4\", trim(col(\"picks_4\")))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks_5\", trim(col(\"picks_5\")))\n",
      "```\n",
      "\n",
      "9. Concat Fields picks\n",
      "```python\n",
      "from pyspark.sql.functions import concat_ws\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"picks\", concat_ws(\" \", col(\"picks_1\"), col(\"picks_2\"), col(\"picks_3\"), col(\"picks_4\"), col(\"picks_5\")))\n",
      "```\n",
      "\n",
      "10. Constant sales factor\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"c_totosf\", lit(0.968804495))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"c_4dbsf\", lit(0.977691588))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"c_4dssf\", lit(0.972523364))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"c_4dbssf\", lit(0.975107476))\n",
      "```\n",
      "\n",
      "11. Formula\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"bet_status\", expr(\"IF((status == 'A' OR status == 'S' OR status == 'X'), status, ob_status)\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"product_id\", expr(\"IF(LEFT(bet_type, 1) == '4', '009', IF(LEFT(bet_type, 1) == 'L', '023', '999'))\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"sales_amount\", expr(\"IF((type_stake_1 == 'TOTO'), c_totosf * total_stake, IF((type_stake_1 == '4DBIG' AND type_stake_2 == ''), c_4dbsf * total_stake, IF((type_stake_1 == '' AND type_stake_2 == '4DSMALL'), c_4dssf * total_stake, IF((type_stake_1 == '4DBIG' AND type_stake_2 == '4DSMALL'), c_4dbssf * total_stake))))\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"gst_amount\", expr(\"total_stake - sales_amount\"))\n",
      "```\n",
      "\n",
      "12. User Defined Java Expression\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"valid_date\", expr(\"IF(returns > 0, settled_date, null)\"))\n",
      "```\n",
      "\n",
      "13. Duplicate Date & Time fields\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"creation_date_d\", col(\"creation_date\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"creation_date_t\", col(\"creation_date\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"settled_date_d\", col(\"settled_date\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"settled_date_t\", col(\"settled_date\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"draw_date_d\", col(\"draw_date\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"draw_date_t\", col(\"draw_date\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"valid_date_d\", col(\"valid_date\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"valid_date_t\", col(\"valid_date\"))\n",
      "```\n",
      "\n",
      "14. Format Date & Time fields\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"business_day\", date_format(\"business_day\", \"yyyyMMdd\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"creation_date_d\", date_format(\"creation_date_d\", \"yyyyMMdd\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"creation_date_t\", date_format(\"creation_date_t\", \"HH:mm:ss\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"settled_date_d\", date_format(\"settled_date_d\", \"yyyyMMdd\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"settled_date_t\", date_format(\"settled_date_t\", \"HH:mm:ss\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"draw_date_d\", date_format(\"draw_date_d\", \"yyyyMMdd\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"draw_date_t\", date_format(\"draw_date_t\", \"HH:mm:ss\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"valid_date_d\", date_format(\"valid_date_d\", \"yyyyMMdd\"))\n",
      "vsapbwlottery = vsapbwlottery.withColumn(\"valid_date_t\", date_format(\"valid_date_t\", \"HH:mm:ss\"))\n",
      "```\n",
      "\n",
      "15. If field value is null\n",
      "```python\n",
      "vsapbwlottery = vsapbwlottery.fillna({\"es_syndicate_id\": 0, \"es_syndicate_name\": \" \", \"es_syndicate_parts\": 0})\n",
      "```\n",
      "\n",
      "16. Filter rows for Cancel\n",
      "```python\n",
      "cancel_rows = vsapbwlottery.filter((col(\"settled_how\") == \"M\") & (col(\"bet_status\") == \"X\"))\n",
      "```\n",
      "\n",
      "17. Memory Group by Cancel\n",
      "```python\n",
      "from pyspark.sql.functions import sum as _sum, count as _count\n",
      "cancel_grouped = cancel_rows.groupBy(\"business_day\", \"account_no\", \"bet_name\", \"bet_status\", \"channel\", \"draw_date_d\", \"es_draw_no\", \"bet_type\", \"auto_pick\", \"es_syndicate_parts\", \"product_id\", \"es_syndicate_id\", \"es_syndicate_name\").agg(_sum(\"bet_amount\").alias(\"bet_amount\"), _sum(\"big_stake_board\").alias(\"big_stake_board\"), _sum(\"big_stake_unit\").alias(\"big_stake_unit\"), _sum(\"input_output_gst_amount\").alias(\"input_output_gst_amount\"), _sum(\"number_of_sold_syndicate_units\").alias(\"number_of_sold_syndicate_units\"), _sum(\"aggr_sales_amount\").alias(\"aggr_sales_amount\"), _sum(\"small_stake_board\").alias(\"small_stake_board\"), _sum(\"small_stake_unit\").alias(\"small_stake_unit\"), _count(\"*\").alias(\"transaction_count\"))\n",
      "```\n",
      "\n",
      "18. Avoid update empty row - Cancel\n",
      "```python\n",
      "cancel_grouped = cancel_grouped.filter((col(\"business_day\").isNotNull()) & (col(\"account_no\").isNotNull()))\n",
      "```\n",
      "\n",
      "19. Insert/Update ztob_lottery_cancel\n",
      "```python\n",
      "cancel_grouped.write.mode(\"overwrite\").parquet(\"maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_lottery_cancel\")\n",
      "```\n",
      "\n",
      "20. Filter rows for Valid\n",
      "```python\n",
      "valid_rows = vsapbwlottery.filter((col(\"bet_status\") == \"S\") & (col(\"valid_date_d\").isNotNull()) & (col(\"returns\") > 0))\n",
      "```\n",
      "\n",
      "21. Memory Group by Valid\n",
      "```python\n",
      "valid_grouped = valid_rows.groupBy(\"business_day\", \"account_no\", \"bet_name\", \"bet_status\", \"channel\", \"draw_date_d\", \"es_draw_no\", \"bet_type\", \"auto_pick\", \"es_syndicate_parts\", \"product_id\", \"es_syndicate_id\", \"es_syndicate_name\").agg(_sum(\"returns\").alias(\"returns\"), _sum(\"winnings\").alias(\"winnings\"), _count(\"*\").alias(\"transaction_count\"))\n",
      "```\n",
      "\n",
      "22. Avoid update empty row - Valid\n",
      "```python\n",
      "valid_grouped = valid_grouped.filter((col(\"business_day\").isNotNull()) & (col(\"account_no\").isNotNull()))\n",
      "```\n",
      "\n",
      "23. Insert/Update ztob_lottery_valid\n",
      "```python\n",
      "valid_grouped.write.mode(\"overwrite\").parquet(\"maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_lottery_valid\")\n",
      "```\n",
      "\n",
      "24. Memory Group by Wager\n",
      "```python\n",
      "wager_grouped = vsapbwlottery.groupBy(\"business_day\", \"account_no\", \"bet_name\", \"bet_status\", \"channel\", \"draw_date_d\", \"es_draw_no\", \"bet_type\", \"auto_pick\", \"es_syndicate_parts\", \"product_id\", \"es_syndicate_id\", \"es_syndicate_name\").agg(_sum(\"bet_amount\").alias(\"bet_amount\"), _sum(\"big_stake_board\").alias(\"big_stake_board\"), _sum(\"big_stake_unit\").alias(\"big_stake_unit\"), _sum(\"input_output_gst_amount\").alias(\"input_output_gst_amount\"), _sum(\"number_of_sold_syndicate_units\").alias(\"number_of_sold_syndicate_units\"), _sum(\"aggr_sales_amount\").alias(\"aggr_sales_amount\"), _sum(\"small_stake_board\").alias(\"small_stake_board\"), _sum(\"small_stake_unit\").alias(\"small_stake_unit\"), _count(\"*\").alias(\"transaction_count\"))\n",
      "```\n",
      "\n",
      "25. Avoid update empty row - Wager\n",
      "```python\n",
      "wager_grouped = wager_grouped.filter((col(\"business_day\").isNotNull()) & (col(\"account_no\").isNotNull()))\n",
      "```\n",
      "\n",
      "26. Insert/Update ztob_lottery_wager\n",
      "```python\n",
      "wager_grouped.write.mode(\"overwrite\").parquet(\"maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_lottery_wager\")\n",
      "```\n",
      "\n",
      "27. OB006.2.1 MapR - ztob_lottery\n",
      "```python\n",
      "vsapbwlottery.write.mode(\"overwrite\").parquet(\"maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_lottery\")\n",
      "```\n",
      "\n",
      "28. OB006.2.2 MapR - ztob_lottery_wager\n",
      "```python\n",
      "wager_grouped.write.mode(\"overwrite\").parquet(\"maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_lottery_wager\")\n",
      "```\n",
      "\n",
      "29. OB006.2.3 MapR - ztob_lottery_cancel\n",
      "```python\n",
      "cancel_grouped.write.mode(\"overwrite\").parquet(\"maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_lottery_cancel\")\n",
      "```\n",
      "\n",
      "30. OB006.2.4 MapR - ztob_lottery_valid\n",
      "```python\n",
      "valid_grouped.write.mode(\"overwrite\").parquet(\"maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_lottery_valid\")\n",
      "```\n",
      "\n",
      "Please note that the above Spark code is written in PySpark and assumes that the SparkSession has been created and named as `spark`. Also, the code assumes that the data is stored in Parquet format. If the data is stored in a different format, the read and write methods would need to be adjusted accordingly.\n",
      "Copied 'hop_order.txt' to 'spark_code\\TR_OB_Lottery_daily'.\n",
      "Copied 'steps.txt' to 'spark_code\\TR_OB_Lottery_daily'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\TR_OB_Lottery_daily'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportBetsSettled_adhoc.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:30:09,321 - INFO - The converted code is saved in spark_code\\TR_OB_SportBetsSettled_adhoc folder\n",
      "2024-06-13 11:30:09,334 - INFO - order elements extracted\n",
      "2024-06-13 11:30:09,334 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:30:09,352 - INFO - step elements extracted\n",
      "2024-06-13 11:30:09,356 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps and their corresponding Spark code would be as follows:\n",
      "\n",
      "1. Get Variables\n",
      "```scala\n",
      "val fr_date = spark.sqlContext.getConf(\"spark.fr_date\")\n",
      "val to_date = spark.sqlContext.getConf(\"spark.to_date\")\n",
      "```\n",
      "\n",
      "2. Table input vSapBwSportBetsSettled\n",
      "```scala\n",
      "val df = spark.read.format(\"jdbc\")\n",
      "  .option(\"url\", \"jdbc:postgresql://localhost:5432/OpenBet Sports\")\n",
      "  .option(\"dbtable\", \"(SELECT * FROM vsapbwsportbetssettled WHERE creation_date BETWEEN \" + fr_date + \" AND \" + to_date + \") as tmp\")\n",
      "  .option(\"user\", \"username\")\n",
      "  .option(\"password\", \"password\")\n",
      "  .load()\n",
      "```\n",
      "\n",
      "3. DB lookup ztob_businessday\n",
      "```scala\n",
      "val businessDayDF = spark.read.format(\"jdbc\")\n",
      "  .option(\"url\", \"jdbc:postgresql://localhost:5432/ABCDs Qlik Postgres\")\n",
      "  .option(\"dbtable\", \"ztob_businessday\")\n",
      "  .option(\"user\", \"username\")\n",
      "  .option(\"password\", \"password\")\n",
      "  .load()\n",
      "\n",
      "val dfWithBusinessDay = df.join(businessDayDF, df(\"start_time\") <= businessDayDF(\"creation_date\") && df(\"end_time\") >= businessDayDF(\"creation_date\"))\n",
      "```\n",
      "\n",
      "4. Replace in string\n",
      "```scala\n",
      "val dfReplaced = dfWithBusinessDay.withColumn(\"market_name_1\", regexp_replace($\"market_name_1\", \"\\\\|\", \" \"))\n",
      "  .withColumn(\"market_name_2\", regexp_replace($\"market_name_2\", \"\\\\|\", \" \"))\n",
      "  .withColumn(\"selection_name_1\", regexp_replace($\"selection_name_1\", \"\\\\|\", \" \"))\n",
      "  .withColumn(\"selection_name_2\", regexp_replace($\"selection_name_2\", \"\\\\|\", \" \"))\n",
      "```\n",
      "\n",
      "5. String operations\n",
      "```scala\n",
      "val dfTrimmed = dfReplaced.withColumn(\"market_name_1\", trim($\"market_name_1\"))\n",
      "  .withColumn(\"market_name_2\", trim($\"market_name_2\"))\n",
      "  .withColumn(\"selection_name_1\", trim($\"selection_name_1\"))\n",
      "  .withColumn(\"selection_name_2\", trim($\"selection_name_2\"))\n",
      "  .withColumn(\"bet_type\", trim($\"bet_type\"))\n",
      "  .withColumn(\"last_settled_info_1\", trim($\"last_settled_info_1\"))\n",
      "  .withColumn(\"last_settled_info_2\", trim($\"last_settled_info_2\"))\n",
      "  .withColumn(\"last_settled_info_3\", trim($\"last_settled_info_3\"))\n",
      "  .withColumn(\"last_settled_info_4\", trim($\"last_settled_info_4\"))\n",
      "```\n",
      "\n",
      "6. Concat Fields market_name\n",
      "```scala\n",
      "val dfConcatMarketName = dfTrimmed.withColumn(\"market_name\", concat($\"market_name_1\", lit(\" \"), $\"market_name_2\"))\n",
      "```\n",
      "\n",
      "7. Concat Fields selection_name\n",
      "```scala\n",
      "val dfConcatSelectionName = dfConcatMarketName.withColumn(\"selection_name\", concat($\"selection_name_1\", lit(\" \"), $\"selection_name_2\"))\n",
      "```\n",
      "\n",
      "8. Concat Fields last_settled_info\n",
      "```scala\n",
      "val dfConcatLastSettledInfo = dfConcatSelectionName.withColumn(\"last_settled_info\", concat($\"last_settled_info_1\", lit(\" \"), $\"last_settled_info_2\", lit(\" \"), $\"last_settled_info_3\", lit(\" \"), $\"last_settled_info_4\"))\n",
      "```\n",
      "\n",
      "9. Formula\n",
      "```scala\n",
      "val dfWithMultipleBet = dfConcatLastSettledInfo.withColumn(\"multiple_bet\", when($\"bet_type\" === \"SGL\", \"N\").otherwise(\"Y\"))\n",
      "```\n",
      "\n",
      "10. User Defined Java Expression\n",
      "```scala\n",
      "val dfWithSettledFlag = dfWithMultipleBet.withColumn(\"settled_flag\", when($\"last_settled_date\".isNull, \"N\").otherwise(\"Y\"))\n",
      "```\n",
      "\n",
      "11. Duplicate Date & Time fields\n",
      "```scala\n",
      "val dfWithDuplicateFields = dfWithSettledFlag.withColumn(\"creation_date_d\", $\"creation_date\")\n",
      "  .withColumn(\"creation_date_t\", $\"creation_date\")\n",
      "  .withColumn(\"last_settled_date_d\", $\"last_settled_date\")\n",
      "  .withColumn(\"last_settled_date_t\", $\"last_settled_date\")\n",
      "  .withColumn(\"validation_date_d\", $\"validation_date\")\n",
      "  .withColumn(\"validation_date_t\", $\"validation_date\")\n",
      "```\n",
      "\n",
      "12. Format Date & Time fields\n",
      "```scala\n",
      "val dfFormatted = dfWithDuplicateFields.withColumn(\"business_day\", date_format($\"business_day\", \"yyyyMMdd\"))\n",
      "  .withColumn(\"creation_date_d\", date_format($\"creation_date_d\", \"yyyyMMdd\"))\n",
      "  .withColumn(\"creation_date_t\", date_format($\"creation_date_t\", \"HH:mm:ss\"))\n",
      "  .withColumn(\"last_settled_date_d\", date_format($\"last_settled_date_d\", \"yyyyMMdd\"))\n",
      "  .withColumn(\"last_settled_date_t\", date_format($\"last_settled_date_t\", \"HH:mm:ss\"))\n",
      "  .withColumn(\"validation_date_d\", date_format($\"validation_date_d\", \"yyyyMMdd\"))\n",
      "  .withColumn(\"validation_date_t\", date_format($\"validation_date_t\", \"HH:mm:ss\"))\n",
      "```\n",
      "\n",
      "The remaining steps involve filtering rows and updating/inserting into different tables which would be specific to your database and would require using a combination of DataFrame transformations (filter, groupBy, agg) and database-specific operations (JDBC writes, upserts).\n",
      "Copied 'hop_order.txt' to 'spark_code\\TR_OB_SportBetsSettled_adhoc'.\n",
      "Copied 'steps.txt' to 'spark_code\\TR_OB_SportBetsSettled_adhoc'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\TR_OB_SportBetsSettled_adhoc'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportBetsSettled_daily.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:32:23,718 - INFO - The converted code is saved in spark_code\\TR_OB_SportBetsSettled_daily folder\n",
      "2024-06-13 11:32:23,780 - INFO - order elements extracted\n",
      "2024-06-13 11:32:23,783 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:32:23,792 - INFO - step elements extracted\n",
      "2024-06-13 11:32:23,795 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps and their corresponding Spark code would be as follows:\n",
      "\n",
      "1. Table input vSapBwSportBetsSettled\n",
      "```scala\n",
      "val df = spark.read.format(\"jdbc\")\n",
      "  .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\")\n",
      "  .option(\"dbtable\", \"vsapbwsportbetssettled\")\n",
      "  .option(\"user\", \"username\")\n",
      "  .option(\"password\", \"password\")\n",
      "  .load()\n",
      "```\n",
      "\n",
      "2. DB lookup ztob_businessday\n",
      "```scala\n",
      "val dfLookup = spark.read.format(\"jdbc\")\n",
      "  .option(\"url\", \"jdbc:postgresql://localhost:5432/mydb\")\n",
      "  .option(\"dbtable\", \"ztob_businessday\")\n",
      "  .option(\"user\", \"username\")\n",
      "  .option(\"password\", \"password\")\n",
      "  .load()\n",
      "\n",
      "val dfJoined = df.join(dfLookup, df(\"creation_date\") === dfLookup(\"start_time\") && df(\"creation_date\") === dfLookup(\"end_time\"))\n",
      "```\n",
      "\n",
      "3. Replace in string\n",
      "```scala\n",
      "val dfReplaced = dfJoined.withColumn(\"market_name_1\", regexp_replace(col(\"market_name_1\"), \"\\\\|\", \" \"))\n",
      "  .withColumn(\"market_name_2\", regexp_replace(col(\"market_name_2\"), \"\\\\|\", \" \"))\n",
      "  .withColumn(\"selection_name_1\", regexp_replace(col(\"selection_name_1\"), \"\\\\|\", \" \"))\n",
      "  .withColumn(\"selection_name_2\", regexp_replace(col(\"selection_name_2\"), \"\\\\|\", \" \"))\n",
      "```\n",
      "\n",
      "4. String operations\n",
      "```scala\n",
      "val dfTrimmed = dfReplaced.withColumn(\"market_name_1\", trim(col(\"market_name_1\")))\n",
      "  .withColumn(\"market_name_2\", trim(col(\"market_name_2\")))\n",
      "  .withColumn(\"selection_name_1\", trim(col(\"selection_name_1\")))\n",
      "  .withColumn(\"selection_name_2\", trim(col(\"selection_name_2\")))\n",
      "  .withColumn(\"bet_type\", trim(col(\"bet_type\")))\n",
      "  .withColumn(\"last_settled_info_1\", trim(col(\"last_settled_info_1\")))\n",
      "  .withColumn(\"last_settled_info_2\", trim(col(\"last_settled_info_2\")))\n",
      "  .withColumn(\"last_settled_info_3\", trim(col(\"last_settled_info_3\")))\n",
      "  .withColumn(\"last_settled_info_4\", trim(col(\"last_settled_info_4\")))\n",
      "```\n",
      "\n",
      "5. Concat Fields market_name\n",
      "```scala\n",
      "val dfConcat = dfTrimmed.withColumn(\"market_name\", concat(col(\"market_name_1\"), lit(\" \"), col(\"market_name_2\")))\n",
      "```\n",
      "\n",
      "6. Concat Fields selection_name\n",
      "```scala\n",
      "val dfConcat = dfConcat.withColumn(\"selection_name\", concat(col(\"selection_name_1\"), lit(\" \"), col(\"selection_name_2\")))\n",
      "```\n",
      "\n",
      "7. Concat Fields last_settled_info\n",
      "```scala\n",
      "val dfConcat = dfConcat.withColumn(\"last_settled_info\", concat(col(\"last_settled_info_1\"), lit(\" \"), col(\"last_settled_info_2\"), lit(\" \"), col(\"last_settled_info_3\"), lit(\" \"), col(\"last_settled_info_4\")))\n",
      "```\n",
      "\n",
      "8. Formula\n",
      "```scala\n",
      "val dfFormula = dfConcat.withColumn(\"multiple_bet\", when(col(\"bet_type\") === \"SGL\", \"N\").otherwise(\"Y\"))\n",
      "```\n",
      "\n",
      "9. User Defined Java Expression\n",
      "```scala\n",
      "val dfJavaExpr = dfFormula.withColumn(\"settled_flag\", when(col(\"last_settled_date\").isNull, \"N\").otherwise(\"Y\"))\n",
      "```\n",
      "\n",
      "10. Duplicate Date & Time fields\n",
      "```scala\n",
      "val dfDuplicate = dfJavaExpr.withColumn(\"creation_date_d\", col(\"creation_date\"))\n",
      "  .withColumn(\"creation_date_t\", col(\"creation_date\"))\n",
      "  .withColumn(\"last_settled_date_d\", col(\"last_settled_date\"))\n",
      "  .withColumn(\"last_settled_date_t\", col(\"last_settled_date\"))\n",
      "  .withColumn(\"validation_date_d\", col(\"validation_date\"))\n",
      "  .withColumn(\"validation_date_t\", col(\"validation_date\"))\n",
      "```\n",
      "\n",
      "11. Format Date & Time fields\n",
      "```scala\n",
      "val dfFormatted = dfDuplicate.withColumn(\"business_day\", date_format(col(\"business_day\"), \"yyyyMMdd\"))\n",
      "  .withColumn(\"creation_date_d\", date_format(col(\"creation_date_d\"), \"yyyyMMdd\"))\n",
      "  .withColumn(\"creation_date_t\", date_format(col(\"creation_date_t\"), \"HH:mm:ss\"))\n",
      "  .withColumn(\"last_settled_date_d\", date_format(col(\"last_settled_date_d\"), \"yyyyMMdd\"))\n",
      "  .withColumn(\"last_settled_date_t\", date_format(col(\"last_settled_date_t\"), \"HH:mm:ss\"))\n",
      "  .withColumn(\"validation_date_d\", date_format(col(\"validation_date_d\"), \"yyyyMMdd\"))\n",
      "  .withColumn(\"validation_date_t\", date_format(col(\"validation_date_t\"), \"HH:mm:ss\"))\n",
      "```\n",
      "\n",
      "The remaining steps involve filtering rows, grouping by memory, and inserting/updating data into various tables. These steps would follow a similar pattern to the steps above, using Spark DataFrame transformations such as `filter()`, `groupBy()`, and `write()`.\n",
      "Copied 'hop_order.txt' to 'spark_code\\TR_OB_SportBetsSettled_daily'.\n",
      "Copied 'steps.txt' to 'spark_code\\TR_OB_SportBetsSettled_daily'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\TR_OB_SportBetsSettled_daily'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportBets_daily.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:35:31,082 - INFO - The converted code is saved in spark_code\\TR_OB_SportBets_daily folder\n",
      "2024-06-13 11:35:31,095 - INFO - order elements extracted\n",
      "2024-06-13 11:35:31,098 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:35:31,105 - INFO - step elements extracted\n",
      "2024-06-13 11:35:31,109 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps and their corresponding Spark code would be:\n",
      "\n",
      "1. Get last updated date\n",
      "```python\n",
      "last_updated_date = spark.sql(\"SELECT MAX(last_updated_date) FROM ztob_sportbets\")\n",
      "```\n",
      "\n",
      "2. Add constants\n",
      "```python\n",
      "from pyspark.sql.functions import lit\n",
      "df = df.withColumn(\"const_minusone\", lit(-1))\n",
      "```\n",
      "\n",
      "3. Get max_minusone\n",
      "```python\n",
      "from pyspark.sql.functions import expr\n",
      "df = df.withColumn(\"max_minusone\", expr(\"date_add(max, const_minusone)\"))\n",
      "```\n",
      "\n",
      "4. Select field\n",
      "```python\n",
      "df = df.select(\"max_minusone\")\n",
      "```\n",
      "\n",
      "5. Table input vSapBwSportBets\n",
      "```python\n",
      "df = spark.sql(\"SELECT * FROM vsapbwsportbets WHERE last_updated_date > max_minusone\")\n",
      "```\n",
      "\n",
      "6. DB lookup ztob_businessday\n",
      "```python\n",
      "df = df.join(spark.table(\"ztob_businessday\"), (df.start_time <= df.creation_date) & (df.end_time >= df.creation_date), how='left')\n",
      "```\n",
      "\n",
      "7. Replace in string\n",
      "```python\n",
      "from pyspark.sql.functions import regexp_replace\n",
      "df = df.withColumn(\"market_name_1\", regexp_replace(df.market_name_1, \"|\", \" \"))\n",
      "df = df.withColumn(\"market_name_2\", regexp_replace(df.market_name_2, \"|\", \" \"))\n",
      "df = df.withColumn(\"selection_name_1\", regexp_replace(df.selection_name_1, \"|\", \" \"))\n",
      "df = df.withColumn(\"selection_name_2\", regexp_replace(df.selection_name_2, \"|\", \" \"))\n",
      "```\n",
      "\n",
      "8. String operations\n",
      "```python\n",
      "df = df.withColumn(\"market_name_1\", df.market_name_1.trim())\n",
      "df = df.withColumn(\"market_name_2\", df.market_name_2.trim())\n",
      "df = df.withColumn(\"selection_name_1\", df.selection_name_1.trim())\n",
      "df = df.withColumn(\"selection_name_2\", df.selection_name_2.trim())\n",
      "df = df.withColumn(\"bet_type\", df.bet_type.trim())\n",
      "```\n",
      "\n",
      "9. Concat Fields market_name\n",
      "```python\n",
      "from pyspark.sql.functions import concat\n",
      "df = df.withColumn(\"market_name\", concat(df.market_name_1, df.market_name_2))\n",
      "```\n",
      "\n",
      "10. Concat Fields selection_name\n",
      "```python\n",
      "df = df.withColumn(\"selection_name\", concat(df.selection_name_1, df.selection_name_2))\n",
      "```\n",
      "\n",
      "11. Formula\n",
      "```python\n",
      "from pyspark.sql.functions import when\n",
      "df = df.withColumn(\"multiple_bet\", when(df.bet_type == \"SGL\", \"N\").otherwise(\"Y\"))\n",
      "```\n",
      "\n",
      "12. Duplicate Date & Time fields\n",
      "```python\n",
      "df = df.withColumn(\"creation_date_d\", df.creation_date)\n",
      "df = df.withColumn(\"creation_date_t\", df.creation_date)\n",
      "```\n",
      "\n",
      "13. Format Date & Time fields\n",
      "```python\n",
      "from pyspark.sql.functions import date_format\n",
      "df = df.withColumn(\"business_day\", date_format(df.business_day, \"yyyyMMdd\"))\n",
      "df = df.withColumn(\"creation_date_d\", date_format(df.creation_date_d, \"yyyyMMdd\"))\n",
      "df = df.withColumn(\"creation_date_t\", date_format(df.creation_date_t, \"HH:mm:ss\"))\n",
      "```\n",
      "\n",
      "14. Insert/Update ztob_sportbets\n",
      "```python\n",
      "# Assuming df2 is the DataFrame representing the ztob_sportbets table\n",
      "df2 = df2.alias(\"a\").join(df.alias(\"b\"), (df2.bet_id == df.bet_id) & (df2.match_id == df.match_id) & (df2.creation_date_d == df.creation_date_d) & (df2.creation_date_t == df.creation_date_t), how='left')\n",
      "for column in df.columns:\n",
      "    df2 = df2.withColumn(column, when(df2[\"b.\" + column].isNull(), df2[\"a.\" + column]).otherwise(df2[\"b.\" + column]))\n",
      "df2 = df2.select([col for col in df2.columns if not col.startswith(\"b.\")])\n",
      "```\n",
      "\n",
      "15. DB lookup ztob_timetable\n",
      "```python\n",
      "df = df.join(spark.table(\"ztob_timetable\"), df.match_id == spark.table(\"ztob_timetable\").match_id, how='left')\n",
      "```\n",
      "\n",
      "16. If field value is null\n",
      "```python\n",
      "df = df.fillna({\"market_no\": \" \", \"sports_type\": \" \"})\n",
      "```\n",
      "\n",
      "17. Memory Group by Wager\n",
      "```python\n",
      "df = df.groupBy(\"business_day\", \"account_no\", \"bet_type\", \"channel\", \"multiple_bet\", \"bir\", \"market_no\", \"match_id\", \"sports_type\").sum(\"stake\", \"gst\", \"sales\")\n",
      "```\n",
      "\n",
      "18. Avoid update empty row - Wager\n",
      "```python\n",
      "df = df.filter(df.business_day.isNotNull() & df.account_no.isNotNull())\n",
      "```\n",
      "\n",
      "19. Insert/Update ztob_sportbets_wager\n",
      "```python\n",
      "# Assuming df3 is the DataFrame representing the ztob_sportbets_wager table\n",
      "df3 = df3.alias(\"a\").join(df.alias(\"b\"), (df3.business_day == df.business_day) & (df3.account_no == df.account_no) & (df3.bet_type == df.bet_type) & (df3.channel == df.channel) & (df3.multiple_bet == df.multiple_bet) & (df3.bir == df.bir) & (df3.market_no == df.market_no) & (df3.match_id == df.match_id) & (df3.sports_type == df.sports_type), how='left')\n",
      "for column in df.columns:\n",
      "    df3 = df3.withColumn(column, when(df3[\"b.\" + column].isNull(), df3[\"a.\" + column]).otherwise(df3[\"b.\" + column]))\n",
      "df3 = df3.select([col for col in df3.columns if not col.startswith(\"b.\")])\n",
      "```\n",
      "\n",
      "20. OB005.2.1 MapR - ztob_sportbets\n",
      "```python\n",
      "df.write.parquet(\"maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_sportbets\", mode=\"overwrite\")\n",
      "```\n",
      "\n",
      "21. OB005.2.2 MapR - ztob_sportbets_wager\n",
      "```python\n",
      "df3.write.parquet(\"maprfs://${ABCD_MAPR_HDFS_CURATED_TEMPORARY}/ztob_sportbets_wager\", mode=\"overwrite\")\n",
      "```\n",
      "\n",
      "Please note that the above Spark code is written in PySpark and assumes that the initial DataFrame is named `df`. Also, the code for the \"Insert/Update\" steps assumes that the existing tables are loaded into DataFrames named `df2` and `df3` respectively. You may need to adjust the code to fit your specific environment and requirements.\n",
      "Copied 'hop_order.txt' to 'spark_code\\TR_OB_SportBets_daily'.\n",
      "Copied 'steps.txt' to 'spark_code\\TR_OB_SportBets_daily'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\TR_OB_SportBets_daily'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\TR_OB_SportReport40.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:38:16,508 - INFO - The converted code is saved in spark_code\\TR_OB_SportReport40 folder\n",
      "2024-06-13 11:38:16,524 - INFO - order elements extracted\n",
      "2024-06-13 11:38:16,528 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:38:16,538 - INFO - step elements extracted\n",
      "2024-06-13 11:38:16,542 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and the steps file, the sequence of steps and their corresponding Spark code would be as follows:\n",
      "\n",
      "1. Get max date\n",
      "```python\n",
      "max_date = spark.sql(\"SELECT MAX(market_settlement_date) max_date FROM ztob_sportreport40\")\n",
      "```\n",
      "\n",
      "2. Get last businessday\n",
      "```python\n",
      "max_busday = spark.sql(\"SELECT MAX(end_timestamp) max_busday FROM ztall_businessday WHERE system = 'OB'\")\n",
      "```\n",
      "\n",
      "3. Merge Join\n",
      "```python\n",
      "from pyspark.sql.functions import col\n",
      "merged_df = max_date.join(max_busday, how='full_outer')\n",
      "```\n",
      "\n",
      "4. Modified Java Script Value\n",
      "```python\n",
      "from pyspark.sql.functions import expr\n",
      "merged_df = merged_df.withColumn(\"max_secondadd1\", expr(\"date_add(max_date, 1)\"))\n",
      "```\n",
      "\n",
      "5. Get selection criteria\n",
      "```python\n",
      "from pyspark.sql.functions import when\n",
      "merged_df = merged_df.withColumn(\"select_from\", when(col(\"fr_date\").isNull(), col(\"max_secondadd1\")).otherwise(col(\"fr_date\")))\n",
      "merged_df = merged_df.withColumn(\"select_to\", when(col(\"to_date\").isNull(), col(\"max_busday\")).otherwise(col(\"to_date\")))\n",
      "```\n",
      "\n",
      "6. Select values\n",
      "```python\n",
      "selected_df = merged_df.select(\"select_from\", \"select_to\")\n",
      "```\n",
      "\n",
      "7. Table input vSapBwSportReport40\n",
      "```python\n",
      "vsapbwsportreport40_df = spark.sql(\"SELECT * FROM vsapbwsportreport40 WHERE market_settlement_date >= select_from AND market_settlement_date <= select_to\")\n",
      "```\n",
      "\n",
      "8. Add constants\n",
      "```python\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"system\", lit(\"OB\"))\n",
      "```\n",
      "\n",
      "9. Placement_time business_id\n",
      "```python\n",
      "# Assuming ztall_businessday is a DataFrame loaded from the ztall_businessday table\n",
      "placement_time_business_id = ztall_businessday.filter((col(\"system\") == \"OB\") & (col(\"start_timestamp\") <= col(\"placement_time\")) & (col(\"end_timestamp\") >= col(\"placement_time\"))).select(col(\"business_id\").alias(\"placement_time_business_id\"))\n",
      "```\n",
      "\n",
      "10. Market_settlement_date business_id\n",
      "```python\n",
      "market_settlement_date_business_id = ztall_businessday.filter((col(\"system\") == \"OB\") & (col(\"start_timestamp\") <= col(\"market_settlement_date\")) & (col(\"end_timestamp\") >= col(\"market_settlement_date\"))).select(col(\"business_id\").alias(\"market_settlement_date_business_id\"))\n",
      "```\n",
      "\n",
      "11. Replace in string\n",
      "```python\n",
      "from pyspark.sql.functions import regexp_replace\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"match_name_1\", regexp_replace(col(\"match_name_1\"), \"\\\\|\", \" \"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"match_name_2\", regexp_replace(col(\"match_name_2\"), \"\\\\|\", \" \"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"market_name_1\", regexp_replace(col(\"market_name_1\"), \"\\\\|\", \" \"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"market_name_2\", regexp_replace(col(\"market_name_2\"), \"\\\\|\", \" \"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"selection_name_1\", regexp_replace(col(\"selection_name_1\"), \"\\\\|\", \" \"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"selection_name_2\", regexp_replace(col(\"selection_name_2\"), \"\\\\|\", \" \"))\n",
      "```\n",
      "\n",
      "12. String operations\n",
      "```python\n",
      "from pyspark.sql.functions import trim\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"match_name_1\", trim(col(\"match_name_1\")))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"match_name_2\", trim(col(\"match_name_2\")))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"market_name_1\", trim(col(\"market_name_1\")))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"market_name_2\", trim(col(\"market_name_2\")))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"selection_name_1\", trim(col(\"selection_name_1\")))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"selection_name_2\", trim(col(\"selection_name_2\")))\n",
      "```\n",
      "\n",
      "13. Concat Fields match_name\n",
      "```python\n",
      "from pyspark.sql.functions import concat\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"match_name\", concat(col(\"match_name_1\"), col(\"match_name_2\")))\n",
      "```\n",
      "\n",
      "14. Concat Fields market_name\n",
      "```python\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"market_name\", concat(col(\"market_name_1\"), col(\"market_name_2\")))\n",
      "```\n",
      "\n",
      "15. Concat Fields selection_name\n",
      "```python\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"selection_name\", concat(col(\"selection_name_1\"), col(\"selection_name_2\")))\n",
      "```\n",
      "\n",
      "16. Duplicate Date & Time fields\n",
      "```python\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"placement_time_d\", col(\"placement_time\"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"placement_time_t\", col(\"placement_time\"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"market_settlement_date_d\", col(\"market_settlement_date\"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"market_settlement_date_t\", col(\"market_settlement_date\"))\n",
      "```\n",
      "\n",
      "17. Format Date & Time fields\n",
      "```python\n",
      "from pyspark.sql.functions import date_format\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"placement_time_d\", date_format(col(\"placement_time_d\"), \"yyyyMMdd\"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"placement_time_t\", date_format(col(\"placement_time_t\"), \"HH:mm:ss\"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"market_settlement_date_d\", date_format(col(\"market_settlement_date_d\"), \"yyyyMMdd\"))\n",
      "vsapbwsportreport40_df = vsapbwsportreport40_df.withColumn(\"market_settlement_date_t\", date_format(col(\"market_settlement_date_t\"), \"HH:mm:ss\"))\n",
      "```\n",
      "\n",
      "18. Insert/Update ztob_sportreport40\n",
      "```python\n",
      "# Assuming ztob_sportreport40_df is a DataFrame loaded from the ztob_sportreport40 table\n",
      "from pyspark.sql.functions import lit\n",
      "ztob_sportreport40_df = ztob_sportreport40_df.alias(\"a\").join(vsapbwsportreport40_df.alias(\"b\"), (col(\"a.bet_id\") == col(\"b.bet_id\")) & (col(\"a.match_id\") == col(\"b.match_id\")) & (col(\"a.market_id\") == col(\"b.market_id\")) & (col(\"a.selection_id\") == col(\"b.selection_id\")), \"leftouter\")\n",
      "ztob_sportreport40_df = ztob_sportreport40_df.select([when(col(\"b.\" + xx).isNull(), col(\"a.\" + xx)).otherwise(col(\"b.\" + xx)).alias(xx) for xx in ztob_sportreport40_df.columns])\n",
      "```\n",
      "\n",
      "Please note that the above code assumes that the necessary SparkSession and DataFrame objects have been created and that the necessary libraries have been imported. Also, the code for steps 9 and 10 assumes that the ztall_businessday table has been loaded into a DataFrame. The code for step 18 assumes that the ztob_sportreport40 table has been loaded into a DataFrame.\n",
      "Copied 'hop_order.txt' to 'spark_code\\TR_OB_SportReport40'.\n",
      "Copied 'steps.txt' to 'spark_code\\TR_OB_SportReport40'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\TR_OB_SportReport40'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\tr_push_to_postgres.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:39:48,933 - INFO - The converted code is saved in spark_code\\tr_push_to_postgres folder\n",
      "2024-06-13 11:39:48,933 - INFO - order elements extracted\n",
      "2024-06-13 11:39:48,949 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:39:48,956 - INFO - step elements extracted\n",
      "2024-06-13 11:39:48,956 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and steps file, the sequence of steps and their corresponding Spark code would be as follows:\n",
      "\n",
      "1. get var\n",
      "```python\n",
      "# No equivalent Spark code as this is a variable retrieval step in Pentaho\n",
      "```\n",
      "\n",
      "2. tablename\n",
      "```python\n",
      "# No equivalent Spark code as this is a script execution step in Pentaho\n",
      "```\n",
      "\n",
      "3. slct 3\n",
      "```python\n",
      "# No equivalent Spark code as this is a select values step in Pentaho\n",
      "```\n",
      "\n",
      "4. Table exists\n",
      "```python\n",
      "# Spark doesn't have a built-in function to check if a table exists. However, you can use the following code:\n",
      "spark.sql(\"SHOW TABLES LIKE 'tablename'\").count() > 0\n",
      "```\n",
      "\n",
      "5. tbl_not_exist1\n",
      "```python\n",
      "# No equivalent Spark code as this is a filter rows step in Pentaho\n",
      "```\n",
      "\n",
      "6. table_exist\n",
      "```python\n",
      "# No equivalent Spark code as this is a select values step in Pentaho\n",
      "```\n",
      "\n",
      "7. metadata\n",
      "```python\n",
      "# No equivalent Spark code as this is a table input step in Pentaho\n",
      "```\n",
      "\n",
      "8. data_type_change\n",
      "```python\n",
      "# No equivalent Spark code as this is a replace string step in Pentaho\n",
      "```\n",
      "\n",
      "9. prepare_qry\n",
      "```python\n",
      "# No equivalent Spark code as this is a script execution step in Pentaho\n",
      "```\n",
      "\n",
      "10. create_table\n",
      "```python\n",
      "# Spark SQL to create a table\n",
      "spark.sql(\"CREATE TABLE IF NOT EXISTS tablename (col_list)\")\n",
      "```\n",
      "\n",
      "11. block\n",
      "```python\n",
      "# No equivalent Spark code as this is a block until steps finish step in Pentaho\n",
      "```\n",
      "\n",
      "12. create user\n",
      "```python\n",
      "# Spark SQL to grant select on a table to a user\n",
      "spark.sql(\"GRANT SELECT ON tablename TO rolename\")\n",
      "```\n",
      "\n",
      "13. block 2\n",
      "```python\n",
      "# No equivalent Spark code as this is a block until steps finish step in Pentaho\n",
      "```\n",
      "\n",
      "14. postgres_cnfg\n",
      "```python\n",
      "# No equivalent Spark code as this is a table output step in Pentaho\n",
      "```\n",
      "\n",
      "15. schdl_create\n",
      "```python\n",
      "# No equivalent Spark code as this is an execute SQL script step in Pentaho\n",
      "```\n",
      "\n",
      "16. updt end date\n",
      "```python\n",
      "# Spark SQL to update a date in a table\n",
      "spark.sql(\"UPDATE tablename SET date_column = current_date WHERE condition\")\n",
      "```\n",
      "\n",
      "17. insert\n",
      "```python\n",
      "# No equivalent Spark code as this is a transformation executor step in Pentaho\n",
      "```\n",
      "\n",
      "18. Select values\n",
      "```python\n",
      "# No equivalent Spark code as this is a select values step in Pentaho\n",
      "```\n",
      "\n",
      "19. blck\n",
      "```python\n",
      "# No equivalent Spark code as this is a block until steps finish step in Pentaho\n",
      "```\n",
      "\n",
      "20. slct\n",
      "```python\n",
      "# No equivalent Spark code as this is a select values step in Pentaho\n",
      "```\n",
      "\n",
      "21. add_table_in_view\n",
      "```python\n",
      "# No equivalent Spark code as this is a transformation executor step in Pentaho\n",
      "```\n",
      "\n",
      "22. action\n",
      "```python\n",
      "# No equivalent Spark code as this is a constant step in Pentaho\n",
      "```\n",
      "\n",
      "23. slct 5\n",
      "```python\n",
      "# No equivalent Spark code as this is a select values step in Pentaho\n",
      "```\n",
      "\n",
      "24. audit log\n",
      "```python\n",
      "# No equivalent Spark code as this is a transformation executor step in Pentaho\n",
      "```\n",
      "\n",
      "25. Output\n",
      "```python\n",
      "# No equivalent Spark code as this is a dummy step in Pentaho\n",
      "```\n",
      "\n",
      "Please note that not all Pentaho steps have a direct equivalent in Spark. Some steps may require a combination of Spark operations or may not be applicable in a Spark context.\n",
      "Copied 'hop_order.txt' to 'spark_code\\tr_push_to_postgres'.\n",
      "Copied 'steps.txt' to 'spark_code\\tr_push_to_postgres'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\tr_push_to_postgres'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\tr_views_priv_remove.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:40:30,923 - INFO - The converted code is saved in spark_code\\tr_views_priv_remove folder\n",
      "2024-06-13 11:40:30,935 - INFO - order elements extracted\n",
      "2024-06-13 11:40:30,938 - INFO - The output has been saved to 'hop_order.txt'\n",
      "2024-06-13 11:40:30,945 - INFO - step elements extracted\n",
      "2024-06-13 11:40:30,950 - INFO - The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and the steps file, the sequence of steps is as follows:\n",
      "\n",
      "1. Get Variables\n",
      "2. user\n",
      "3. View\n",
      "4. Iselect\n",
      "5. Output\n",
      "\n",
      "Now, let's convert these steps into Spark code:\n",
      "\n",
      "```python\n",
      "# Step 1: Get Variables\n",
      "pviewsave = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"${pviewsave}\")\n",
      "pviewuser = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"${pviewuser}\")\n",
      "pviewrole = spark.read.format(\"csv\").option(\"header\", \"true\").load(\"${pviewrole}\")\n",
      "\n",
      "# Step 2: user\n",
      "user = pviewuser.withColumn(\"portaluser\", F.split(F.col(\"portaluser\"), \",\"))\n",
      "\n",
      "# Step 3: View\n",
      "view = pviewsave.withColumn(\"pagelist\", F.split(F.col(\"pagelistt\"), \",\"))\n",
      "\n",
      "# Step 4: Iselect\n",
      "iselect = view.select(\"pagelist\", \"portaluser_1\")\n",
      "\n",
      "# Step 5: Output\n",
      "output = iselect.write.format(\"jdbc\").option(\"url\", \"jdbc:postgresql:ABCD\").option(\"dbtable\", \"dim_viewprivilige\").option(\"user\", \"username\").option(\"password\", \"password\").save()\n",
      "```\n",
      "\n",
      "Please replace `\"username\"` and `\"password\"` with your actual database username and password. Also, replace `\"jdbc:postgresql:ABCD\"` with your actual JDBC URL.\n",
      "\n",
      "Please note that this is a simplified conversion and may not cover all the functionalities of the original Pentaho code. For example, the SQL query in the Output step is not converted because Spark does not support executing SQL queries directly on external databases. You may need to use a JDBC connection for that, or convert the SQL logic into Spark transformations.\n",
      "Copied 'hop_order.txt' to 'spark_code\\tr_views_priv_remove'.\n",
      "Copied 'steps.txt' to 'spark_code\\tr_views_priv_remove'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\tr_views_priv_remove'.\n",
      "file_path is C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy\\tr_views_priv_viewlist_updt.ktr\n",
      "The output has been saved to 'hop_order.txt'\n",
      "The output has been saved to 'steps.txt'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-06-13 11:41:30,025 - INFO - The converted code is saved in spark_code\\tr_views_priv_viewlist_updt folder\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Based on the hop order and the steps file, the sequence of steps and their corresponding Spark code would be as follows:\n",
      "\n",
      "1. Get Variables\n",
      "```scala\n",
      "val pviewsave = spark.sqlContext.getConf(\"pviewsave\")\n",
      "val pviewuser = spark.sqlContext.getConf(\"pviewuser\")\n",
      "val pviewrole = spark.sqlContext.getConf(\"pviewrole\")\n",
      "```\n",
      "\n",
      "2. Filter rows\n",
      "```scala\n",
      "val filteredDF = df.filter(col(\"pagelistt\").contains(\",\"))\n",
      "```\n",
      "\n",
      "3. DView\n",
      "```scala\n",
      "val splitDF = filteredDF.withColumn(\"pagelist\", explode(split(col(\"pagelistt\"), \",\")))\n",
      "```\n",
      "\n",
      "4. Dselect\n",
      "```scala\n",
      "val selectedDF = splitDF.select(\"pagelist\", \"portaluser\", \"pviewrole\", col(\"pviewrole\").as(\"pviewrole_1\"))\n",
      "```\n",
      "\n",
      "5. delete priviliges\n",
      "```scala\n",
      "spark.sql(\"delete from dim_viewprivilige where view_id in (select view_id from dim_view where business_name = ? and category_id<>6) and user_id in(select a.user_id from dim_user a join dim_viewprivilige b on a.user_id=b.user_id where status=1) and role_id in (select role_id from dim_role where (role_name = ? or 'ALL'=?) and role_name<>'dh_superadmin' and status=1)\")\n",
      "```\n",
      "\n",
      "6. block\n",
      "```scala\n",
      "// This step is not applicable in Spark as it is used for synchronization in Pentaho\n",
      "```\n",
      "\n",
      "7. user\n",
      "```scala\n",
      "val userDF = df.withColumn(\"portaluser\", explode(split(col(\"portaluser\"), \",\")))\n",
      "```\n",
      "\n",
      "8. View\n",
      "```scala\n",
      "val viewDF = userDF.withColumn(\"pagelist\", explode(split(col(\"pagelistt\"), \",\")))\n",
      "```\n",
      "\n",
      "9. Iselect\n",
      "```scala\n",
      "val finalDF = viewDF.select(\"pagelist\", \"portaluser_1\")\n",
      "```\n",
      "\n",
      "10. Output\n",
      "```scala\n",
      "finalDF.write.format(\"jdbc\").option(\"url\", \"jdbc:postgresql:dbserver\").option(\"dbtable\", \"schema.tablename\").option(\"user\", \"username\").option(\"password\", \"password\").save()\n",
      "```\n",
      "\n",
      "Please note that the actual Spark code might vary depending on the exact data types and structures of your dataframes and the specific configurations of your Spark and database environments.\n",
      "Copied 'hop_order.txt' to 'spark_code\\tr_views_priv_viewlist_updt'.\n",
      "Copied 'steps.txt' to 'spark_code\\tr_views_priv_viewlist_updt'.\n",
      "Copied 'pyspark_code.txt' to 'spark_code\\tr_views_priv_viewlist_updt'.\n"
     ]
    }
   ],
   "source": [
    "def process_directory(directory_path):\n",
    "    # Iterate over all files in the directory\n",
    "    for filename in os.listdir(directory_path):\n",
    "        # Construct the full file path\n",
    "        file_path = os.path.join(directory_path, filename)\n",
    "        \n",
    "        # Check if it's a file and not a directory\n",
    "        if os.path.isfile(file_path):\n",
    "            # Process the file based on its extension\n",
    "            if file_path.endswith('.ktr'):\n",
    "                print('file_path is', file_path)\n",
    "                file_name = os.path.basename(file_path)\n",
    "                folder_name = os.path.splitext(file_name)[0]\n",
    "                process_ktr(file_path, folder_name)\n",
    "\n",
    "# Start the processing with the main directory\n",
    "main_directory_path = r'C:\\Multiple_KTR_Folder\\KTR_Folder\\Medium&Easy'  # Replace with the path to your directory\n",
    "process_directory(main_directory_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
